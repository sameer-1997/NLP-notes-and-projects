{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK - basics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXAMPLE_TEXT = \"Hello Mr. Smith, how are you doing today? \\\n",
    "               The weather is great, and Python is awesome !\\\n",
    "               The sky is pinkish-blue. \\\n",
    "               You shouldn\\'t eat cardboard.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"\"\"At eight o'clock on Thursday morning Arthur felt very good. But he didn't go to play\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentence tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello Mr. Smith, how are you doing today?',\n",
       " 'The weather is great, and Python is awesome !',\n",
       " 'The sky is pinkish-blue.',\n",
       " \"You shouldn't eat cardboard.\"]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(EXAMPLE_TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello Mr. Smith, how are you doing today?\n",
      "The weather is great, and Python is awesome !\n",
      "The sky is pinkish-blue.\n",
      "You shouldn't eat cardboard.\n"
     ]
    }
   ],
   "source": [
    "for sent in sent_tokenize(EXAMPLE_TEXT):\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So there, we have created tokens, which are sentences. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['At',\n",
       " 'eight',\n",
       " \"o'clock\",\n",
       " 'on',\n",
       " 'Thursday',\n",
       " 'morning',\n",
       " 'Arthur',\n",
       " 'felt',\n",
       " 'very',\n",
       " 'good',\n",
       " '.',\n",
       " 'But',\n",
       " 'he',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'go',\n",
       " 'to',\n",
       " 'play']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.word_tokenize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Hello',\n",
       " 'Mr.',\n",
       " 'Smith',\n",
       " ',',\n",
       " 'how',\n",
       " 'are',\n",
       " 'you',\n",
       " 'doing',\n",
       " 'today',\n",
       " '?',\n",
       " 'The',\n",
       " 'weather',\n",
       " 'is',\n",
       " 'great',\n",
       " ',',\n",
       " 'and',\n",
       " 'Python',\n",
       " 'is',\n",
       " 'awesome',\n",
       " '!',\n",
       " 'The',\n",
       " 'sky',\n",
       " 'is',\n",
       " 'pinkish-blue',\n",
       " '.',\n",
       " 'You',\n",
       " 'should',\n",
       " \"n't\",\n",
       " 'eat',\n",
       " 'cardboard',\n",
       " '.']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(EXAMPLE_TEXT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Observation. \n",
    "- First, notice that punctuation is treated as a separate token. \n",
    "- Also, notice the separation of the word \"shouldn't\" into \"should\" and \"n't.\" \n",
    "- Finally, notice that \"pinkish-blue\" is indeed treated like the \"one word\" it was meant to be turned into\n",
    "\n",
    "- Some words seem trivial - these are a form of \"stop words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"this is Ram's text, is'nt it?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', \"Ram's\", 'text,', \"is'nt\", 'it?']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'Ram', \"'s\", 'text', ',', \"is'nt\", 'it', '?']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['this', 'is', 'Ram', \"'\", 's', 'text', ',', 'is', \"'\", 'nt', 'it', '?']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
    "tokenizer.tokenize(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'we', \"that'll\", 'what', 'have', 'did', 'so', 'had', 'are', \"shan't\", 'herself', 'hasn', 'll', 'our', \"mustn't\", \"you'll\", 're', 'other', 'before', 'doing', 'a', 'yourself', 'again', 'i', 'nor', 'from', 'ma', 'the', 'this', 'into', 'now', 'him', 'as', \"won't\", 'or', 't', 'than', \"it's\", 'itself', 'hadn', 'an', 'their', 'weren', \"don't\", 've', 'some', 'hers', 'ours', 'up', 'no', 'most', 'in', 'myself', 'on', 'too', 'mightn', 'she', 'until', 'ain', \"wouldn't\", 'having', 'will', 'don', 'theirs', 'down', \"hadn't\", 'when', 'can', 'further', \"didn't\", 'each', 'those', \"aren't\", 'where', 'you', \"isn't\", 'was', 'at', 'which', 'here', 'wasn', 'through', 'these', 'of', 'by', 'ourselves', 'same', \"shouldn't\", 'with', \"weren't\", 'why', 'haven', 'being', 'any', 'under', 'my', 'for', 's', 'over', 'how', 'isn', 'because', 'needn', 'and', \"wasn't\", 'am', 'y', 'against', 'between', 'won', 'is', 'more', 'it', 'himself', \"you'd\", 'both', 'me', 'do', 'been', \"hasn't\", 'yourselves', 'only', 'themselves', 'out', 'were', 'if', 'during', 'below', 'just', 'yours', 'there', 'them', 'wouldn', \"she's\", 'does', 'off', \"you're\", \"should've\", \"doesn't\", 'above', 'they', \"haven't\", 'shan', 'to', 'whom', 'after', 'then', 'shouldn', 'all', 'be', 'his', 'own', 'd', 'he', 'such', 'while', \"couldn't\", 'doesn', 'not', 'very', 'm', 'her', 'but', 'mustn', 'o', 'aren', 'your', 'couldn', 'has', \"you've\", 'should', 'that', \"mightn't\", \"needn't\", 'about', 'its', 'once', 'few', 'didn', 'who'}\n"
     ]
    }
   ],
   "source": [
    "print(set(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_sent = \"This is a sample sentence, showing off the stop words filtration.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokens = word_tokenize(example_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "['This', 'sample', 'sentence', ',', 'showing', 'stop', 'words', 'filtration', '.']\n"
     ]
    }
   ],
   "source": [
    "# option 1\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "# option 2\n",
    "filtered_sentence = []\n",
    "\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "\n",
    "print(word_tokens)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming words\n",
    "\n",
    "The idea of stemming is a sort of normalizing method. Many variations of words carry the same meaning, other than when tense is involved.\n",
    "\n",
    "The reason why we stem is to shorten the lookup, and normalize sentences.\n",
    "\n",
    "Consider:\n",
    "\n",
    "I was taking a ride in the car.\n",
    "I was riding in the car.\n",
    "\n",
    "One of the most popular stemming algorithms is the __Porter stemmer__, which has been around since 1979."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.stem import SnowballStemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "porter   = PorterStemmer()\n",
    "lancaster=LancasterStemmer()\n",
    "sno      = nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                 Porter Stemmer       lancaster Stemmer    lancaster Stemmer   \n",
      "connected            connect              connect              connect             \n",
      "connecting           connect              connect              connect             \n",
      "connection           connect              connect              connect             \n",
      "connections          connect              connect              connect             \n"
     ]
    }
   ],
   "source": [
    "word_list = [\"connected\", \"connecting\", \"connection\", \"connections\"]\n",
    "\n",
    "print(\"{0:20} {1:20} {2:20} {2:20}\".format(\"Word\",\"Porter Stemmer\", \"lancaster Stemmer\", \"Snowball Stemmer\"))\n",
    "for word in word_list:\n",
    "    print(\"{0:20} {1:20} {2:20} {2:20}\".format(word, porter.stem(word), lancaster.stem(word), sno.stem(word)))\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                 Porter Stemmer       lancaster Stemmer    lancaster Stemmer   \n",
      "run                  run                  run                  run                 \n",
      "running              run                  run                  run                 \n",
      "runs                 run                  run                  run                 \n",
      "runner               runner               run                  run                 \n",
      "monthly              monthli              month                month               \n"
     ]
    }
   ],
   "source": [
    "word_list = [\"run\", \"running\", \"runs\", \"runner\", \"monthly\"]\n",
    "\n",
    "print(\"{0:20} {1:20} {2:20} {2:20}\".format(\"Word\",\"Porter Stemmer\", \"lancaster Stemmer\", \"Snowball Stemmer\"))\n",
    "for word in word_list:\n",
    "    print(\"{0:20} {1:20} {2:20} {2:20}\".format(word, porter.stem(word), lancaster.stem(word), sno.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                 Porter Stemmer       lancaster Stemmer    lancaster Stemmer   \n",
      "cats                 cat                  cat                  cat                 \n",
      "trouble              troubl               troubl               troubl              \n",
      "troubling            troubl               troubl               troubl              \n",
      "troubled             troubl               troubl               troubl              \n",
      "troublesome          troublesom           troublesom           troublesom          \n"
     ]
    }
   ],
   "source": [
    "word_list = [\"cats\", \"trouble\", \"troubling\", \"troubled\", \"troublesome\"]\n",
    "\n",
    "print(\"{0:20} {1:20} {2:20} {2:20}\".format(\"Word\",\"Porter Stemmer\", \"lancaster Stemmer\", \"Snowball Stemmer\"))\n",
    "for word in word_list:\n",
    "    print(\"{0:20} {1:20} {2:20} {2:20}\".format(word, porter.stem(word), lancaster.stem(word), sno.stem(word))) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the PorterStemmer is \n",
    "- giving the root (stem) of the word \"cats\" by simply removing the 's' after cat. This is a suffix added to cat to make it plural. \n",
    "- But if we look at 'trouble', 'troubling' and 'troubled' they are stemmed to 'trouble' because **PorterStemmer algorithm does not follow linguistics rather a set of 05 rules for different cases that are applied in phases (step by step) to generate stems**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                 Porter Stemmer       lancaster Stemmer    lancaster Stemmer   \n",
      "argue                argu                 argu                 argu                \n",
      "argued               argu                 argu                 argu                \n",
      "argues               argu                 argu                 argu                \n",
      "arguing              argu                 argu                 argu                \n",
      "argus                argu                 arg                  arg                 \n"
     ]
    }
   ],
   "source": [
    "word_list = [\"argue\", \"argued\", \"argues\", \"arguing\", \"argus\"]\n",
    "\n",
    "print(\"{0:20} {1:20} {2:20} {2:20}\".format(\"Word\",\"Porter Stemmer\", \"lancaster Stemmer\", \"Snowball Stemmer\"))\n",
    "for word in word_list:\n",
    "    print(\"{0:20} {1:20} {2:20} {2:20}\".format(word, porter.stem(word), lancaster.stem(word), sno.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                 Porter Stemmer       lancaster Stemmer    lancaster Stemmer   \n",
      "friend               friend               friend               friend              \n",
      "friendship           friendship           friend               friend              \n",
      "friends              friend               friend               friend              \n",
      "friendships          friendship           friend               friend              \n",
      "stabil               stabil               stabl                stabl               \n",
      "destabilize          destabil             dest                 dest                \n",
      "misunderstanding     misunderstand        misunderstand        misunderstand       \n",
      "railroad             railroad             railroad             railroad            \n",
      "moonlight            moonlight            moonlight            moonlight           \n",
      "football             footbal              footbal              footbal             \n"
     ]
    }
   ],
   "source": [
    "#A list of words to be stemmed\n",
    "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabil\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\"football\"]\n",
    "\n",
    "print(\"{0:20} {1:20} {2:20} {2:20}\".format(\"Word\",\"Porter Stemmer\", \"lancaster Stemmer\", \"Snowball Stemmer\"))\n",
    "for word in word_list:\n",
    "    print(\"{0:20} {1:20} {2:20} {2:20}\".format(word, porter.stem(word), lancaster.stem(word), sno.stem(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pythoners are very intelligent and work very pythonly and now they are pythoning their way to success.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence=\"Pythoners are very intelligent and work very pythonly and now they are pythoning their way to success.\"\n",
    "porter.stem(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stemmer sees the entire sentence as a word, so it returns it as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My system keep crash hi crash yesterday, our crash daili\n",
      "my system keep crash his crash yesterday, our crash dai\n"
     ]
    }
   ],
   "source": [
    "text = \"My system keeps crashing his crashed yesterday, ours crashes daily\"\n",
    "\n",
    "print(' '.join([porter.stem(word) for word in text.split()]))\n",
    "print(' '.join([lancaster.stem(word) for word in text.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'simple_stemmer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-28-7832dfb132a7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msimple_stemmer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"My system keeps crashing his crashed yesterday, ours crashes daily\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'simple_stemmer' is not defined"
     ]
    }
   ],
   "source": [
    "simple_stemmer(\"My system keeps crashing his crashed yesterday, ours crashes daily\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### limitations of porter stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-2215b14faf01>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m' --> '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'ps' is not defined"
     ]
    }
   ],
   "source": [
    "text = ['business', 'busy', 'PROBE', 'PROBATE', 'clip', 'clippings']\n",
    "\n",
    "for w in text:\n",
    "    \n",
    "    print(w, ' --> ', ps.stem(w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lemmatization\n",
    "\n",
    "Lemmatization is the process of converting a word to its base form. \n",
    "\n",
    "The difference between stemming and lemmatization is, \n",
    "\n",
    "> lemmatization considers the context and converts the word to its meaningful base form, whereas stemming just removes the last few characters, often leading to incorrect meanings and spelling errors.\n",
    "\n",
    "For example, lemmatization would correctly identify the base form of ‘caring’ to ‘care’, whereas, stemming would cutoff the ‘ing’ part and convert it to car.\n",
    "\n",
    "    ‘Caring’ -> Lemmatization -> ‘Care’\n",
    "    ‘Caring’ -> Stemming -> ‘Car’\n",
    "    \n",
    "ways to lemmatize:-\n",
    "\n",
    "    Wordnet Lemmatizer\n",
    "    Spacy Lemmatizer\n",
    "    TextBlob\n",
    "    CLiPS Pattern\n",
    "    Stanford CoreNLP\n",
    "    Gensim Lemmatizer\n",
    "    TreeTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Init the Wordnet Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word                 WordNetLemmatizer   \n",
      "friend               friend               \n",
      "friendship           friendship           \n",
      "friends              friend               \n",
      "friendships          friendship           \n",
      "stabilize            stabilize            \n",
      "destabilize          destabilize          \n",
      "misunderstanding     misunderstanding     \n",
      "railroad             railroad             \n",
      "moonlight            moonlight            \n",
      "football             football             \n"
     ]
    }
   ],
   "source": [
    "word_list = [\"friend\", \"friendship\", \"friends\", \"friendships\",\"stabilize\",\"destabilize\",\"misunderstanding\",\"railroad\",\"moonlight\",\"football\"]\n",
    "\n",
    "print(\"{0:20} {1:20}\".format(\"Word\",\"WordNetLemmatizer\"))\n",
    "\n",
    "for word in word_list:\n",
    "    print(\"{0:20} {1:20} \".format(word, lemmatizer.lemmatize(word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bat\n",
      "are\n",
      "foot\n"
     ]
    }
   ],
   "source": [
    "# Lemmatize Single Word\n",
    "print(lemmatizer.lemmatize(\"bats\"))\n",
    "\n",
    "print(lemmatizer.lemmatize(\"are\"))\n",
    "\n",
    "print(lemmatizer.lemmatize(\"feet\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['The', 'striped', 'bats', 'are', 'hanging', 'on', 'their', 'feet', 'for', 'best']\n"
     ]
    }
   ],
   "source": [
    "# Define the sentence to be lemmatized\n",
    "sentence = \"The striped bats are hanging on their feet for best\"\n",
    "\n",
    "# Tokenize: Split the sentence into words\n",
    "word_list = nltk.word_tokenize(sentence)\n",
    "print(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The --> The\n",
      "striped --> striped\n",
      "bats --> bat\n",
      "are --> are\n",
      "hanging --> hanging\n",
      "on --> on\n",
      "their --> their\n",
      "feet --> foot\n",
      "for --> for\n",
      "best --> best\n"
     ]
    }
   ],
   "source": [
    "for w in word_list:\n",
    "    print(w, '-->', lemmatizer.lemmatize(w) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice it didn’t do a good job. Because, ‘are’ is not converted to ‘be’ and ‘hanging’ is not converted to ‘hang’ as expected. \n",
    "\n",
    "This can be corrected if we provide the correct ‘part-of-speech’ tag (POS tag) as the second argument to lemmatize()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "strip\n",
      "stripe\n"
     ]
    }
   ],
   "source": [
    "print(lemmatizer.lemmatize(\"stripes\", 'v')) \n",
    "print(lemmatizer.lemmatize(\"stripes\", 'n'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the N-grams for the given sentence\n",
    "\n",
    "The essential concepts in text mining is n-grams, which are a set of co-occurring or continuous sequence of n items from a sequence of large text or sentence. The item here could be words, letters, and syllables. 1-gram is also called as unigrams are the unique words present in the sentence. Bigram(2-gram) is the combination of 2 words. Trigram(3-gram) is 3 words and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Data science is an interesting field of study, includes ML and DL as sub field'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "grams = 2\n",
    "\n",
    "n_grams = ngrams(nltk.word_tokenize(text), grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object ngrams at 0x0000028A525E6A98>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Data', 'science'),\n",
       " ('science', 'is'),\n",
       " ('is', 'an'),\n",
       " ('an', 'interesting'),\n",
       " ('interesting', 'field'),\n",
       " ('field', 'of'),\n",
       " ('of', 'study'),\n",
       " ('study', ','),\n",
       " (',', 'includes'),\n",
       " ('includes', 'ML'),\n",
       " ('ML', 'and'),\n",
       " ('and', 'DL'),\n",
       " ('DL', 'as'),\n",
       " ('as', 'sub'),\n",
       " ('sub', 'field')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.bigrams(word_tokenize(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Data science',\n",
       " 'science is',\n",
       " 'is an',\n",
       " 'an interesting',\n",
       " 'interesting field',\n",
       " 'field of',\n",
       " 'of study',\n",
       " 'study ,',\n",
       " ', includes',\n",
       " 'includes ML',\n",
       " 'ML and',\n",
       " 'and DL',\n",
       " 'DL as',\n",
       " 'as sub',\n",
       " 'sub field']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[' '.join(grams) for grams in n_grams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'Data science is a wonderful program, \\\n",
    "Data science is a land of opportunities,data science is about machine learning '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigrams = list(nltk.bigrams(word_tokenize(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({('Data', 'science'): 2,\n",
       "         ('science', 'is'): 3,\n",
       "         ('is', 'a'): 2,\n",
       "         ('a', 'wonderful'): 1,\n",
       "         ('wonderful', 'program'): 1,\n",
       "         ('program', ','): 1,\n",
       "         (',', 'Data'): 1,\n",
       "         ('a', 'land'): 1,\n",
       "         ('land', 'of'): 1,\n",
       "         ('of', 'opportunities'): 1,\n",
       "         ('opportunities', ','): 1,\n",
       "         (',', 'data'): 1,\n",
       "         ('data', 'science'): 1,\n",
       "         ('is', 'about'): 1,\n",
       "         ('about', 'machine'): 1,\n",
       "         ('machine', 'learning'): 1})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Counter(bigrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OR equivalenty ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1-gram:  ['A', 'class', 'is', 'a', 'blueprint', 'for', 'the', 'object', '.']\n",
      "2-gram:  ['A class', 'class is', 'is a', 'a blueprint', 'blueprint for', 'for the', 'the object', 'object .']\n",
      "3-gram:  ['A class is', 'class is a', 'is a blueprint', 'a blueprint for', 'blueprint for the', 'for the object', 'the object .']\n",
      "4-gram:  ['A class is a', 'class is a blueprint', 'is a blueprint for', 'a blueprint for the', 'blueprint for the object', 'for the object .']\n"
     ]
    }
   ],
   "source": [
    "# Function to generate n-grams from sentences.\n",
    "def extract_ngrams(data, num):\n",
    "    n_grams = ngrams(nltk.word_tokenize(data), num)\n",
    "    return [ ' '.join(grams) for grams in n_grams]\n",
    " \n",
    "data = 'A class is a blueprint for the object.'\n",
    " \n",
    "print(\"1-gram: \", extract_ngrams(data, 1))\n",
    "print(\"2-gram: \", extract_ngrams(data, 2))\n",
    "print(\"3-gram: \", extract_ngrams(data, 3))\n",
    "print(\"4-gram: \", extract_ngrams(data, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Data', 'science', 'is'),\n",
       " ('science', 'is', 'an'),\n",
       " ('is', 'an', 'interesting'),\n",
       " ('an', 'interesting', 'field'),\n",
       " ('interesting', 'field', 'of'),\n",
       " ('field', 'of', 'study'),\n",
       " ('of', 'study', ','),\n",
       " ('study', ',', 'includes'),\n",
       " (',', 'includes', 'ML'),\n",
       " ('includes', 'ML', 'and'),\n",
       " ('ML', 'and', 'DL'),\n",
       " ('and', 'DL', 'as'),\n",
       " ('DL', 'as', 'sub'),\n",
       " ('as', 'sub', 'field')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(nltk.trigrams(word_tokenize(text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## parts of speech tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Dummy text \n",
    "txt = '''Sukanya, Rajib and Naba are my good friends. Sukanya is getting married next year. Marriage is a big step in one’s life.\\  \n",
    "       It is both exciting and frightening. But friendship is a sacred bond between people. \\  \n",
    "       It is a special kind of love between us. \\\n",
    "       Many of you must have tried searching for a friend \\\n",
    "       but never found the right one.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized = sent_tokenize(txt) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Sukanya', 'NNP'), (',', ','), ('Rajib', 'NNP'), ('Naba', 'NNP'), ('good', 'JJ'), ('friends', 'NNS'), ('.', '.')]\n",
      "[('Sukanya', 'NNP'), ('getting', 'VBG'), ('married', 'VBN'), ('next', 'JJ'), ('year', 'NN'), ('.', '.')]\n",
      "[('Marriage', 'NN'), ('big', 'JJ'), ('step', 'NN'), ('one', 'CD'), ('’', 'NN'), ('life.\\\\', 'VBZ'), ('It', 'PRP'), ('exciting', 'VBG'), ('frightening', 'VBG'), ('.', '.')]\n",
      "[('But', 'CC'), ('friendship', 'NN'), ('sacred', 'VBD'), ('bond', 'NN'), ('people', 'NNS'), ('.', '.')]\n",
      "[('\\\\', 'VB'), ('It', 'PRP'), ('special', 'JJ'), ('kind', 'NN'), ('love', 'VB'), ('us', 'PRP'), ('.', '.')]\n",
      "[('Many', 'JJ'), ('must', 'MD'), ('tried', 'VB'), ('searching', 'VBG'), ('friend', 'NN'), ('never', 'RB'), ('found', 'VBD'), ('right', 'JJ'), ('one', 'CD'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "for i in tokenized: \n",
    "      \n",
    "    # Word tokenizers is used to find the words  \n",
    "    # and punctuation in a string \n",
    "    wordsList = nltk.word_tokenize(i) \n",
    "  \n",
    "    # removing stop words from wordList \n",
    "    wordsList = [w for w in wordsList if not w in stop_words]  \n",
    "  \n",
    "    #  Using a Tagger. Which is part-of-speech  \n",
    "    # tagger or POS-tagger.  \n",
    "    tagged = nltk.pos_tag(wordsList) \n",
    "  \n",
    "    print(tagged) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
