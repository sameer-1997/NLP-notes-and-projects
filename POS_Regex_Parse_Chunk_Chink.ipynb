{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tag import pos_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex = 'The big yellow bird flew over my house'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = preprocess(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DT'),\n",
       " ('big', 'JJ'),\n",
       " ('yellow', 'JJ'),\n",
       " ('bird', 'NN'),\n",
       " ('flew', 'VBD'),\n",
       " ('over', 'IN'),\n",
       " ('my', 'PRP$'),\n",
       " ('house', 'NN')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JJ: adjective or numeral, ordinal\n",
      "    third ill-mannered pre-war regrettable oiled calamitous first separable\n",
      "    ectoplasmic battery-powered participatory fourth still-to-be-named\n",
      "    multilingual multi-disciplinary ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset(\"JJ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset(\"VBD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NN: noun, common, singular or mass\n",
      "    common-carrier cabbage knuckle-duster Casino afghan shed thermostat\n",
      "    investment slide humour falloff slick wind hyena override subhumanity\n",
      "    machinist ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset(\"NN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VBD: verb, past tense\n",
      "    dipped pleaded swiped regummed soaked tidied convened halted registered\n",
      "    cushioned exacted snubbed strode aimed adopted belied figgered\n",
      "    speculated wore appreciated contemplated ...\n"
     ]
    }
   ],
   "source": [
    "nltk.help.upenn_tagset(\"VBD\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noun Phrase Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_rule = \"NP: {<DT>?<JJ>*<NN>}\"\n",
    "# A noun pharase should be formed when a chunker finds an \n",
    "# optional determiner DT followed by any number of Adjectives JJ and then and Noun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP The/DT big/JJ yellow/JJ bird/NN)\n",
      "  flew/VBD\n",
      "  over/IN\n",
      "  my/PRP$\n",
      "  (NP house/NN))\n"
     ]
    }
   ],
   "source": [
    "cp = nltk.RegexpParser(np_rule)\n",
    "cs = cp.parse(sent)\n",
    "print(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"the little yellow dog barked at cat\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP the/DT little/JJ yellow/JJ dog/NN)\n",
      "  barked/VBD\n",
      "  at/IN\n",
      "  (NP cat/NN))\n"
     ]
    }
   ],
   "source": [
    "cp = nltk.RegexpParser(np_rule)\n",
    "cs = cp.parse(sent)\n",
    "print(cs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "cs.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['the little yellow dog barked at cat']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [nltk.word_tokenize(sent) for sent in sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'little', 'yellow', 'dog', 'barked', 'at', 'cat']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('the', 'DT'), ('little', 'JJ'), ('yellow', 'JJ'), ('dog', 'NN'), ('barked', 'VBD'), ('at', 'IN'), ('cat', 'NN')]]\n"
     ]
    }
   ],
   "source": [
    "sentences = [nltk.pos_tag(sent) for sent in sentences]\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('the', 'DT'),\n",
       "  ('little', 'JJ'),\n",
       "  ('yellow', 'JJ'),\n",
       "  ('dog', 'NN'),\n",
       "  ('barked', 'VBD'),\n",
       "  ('at', 'IN'),\n",
       "  ('cat', 'NN')]]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "grammer = ('''\n",
    "            NP: {<DT>?<JJ>*<NN>} # NP\n",
    "            ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp = nltk.RegexpParser(grammer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<chunk.RegexpParser with 1 stages>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cp"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "result = cp.parse(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "remove numbers : Team A has  batsman and  bowlers, while team b has  batsman and  bowlers\n"
     ]
    }
   ],
   "source": [
    "#Loading the regex package to find number\n",
    "import re\n",
    "\n",
    "#identify user input\n",
    "input_str = \"Team A has 6 batsman and 5 bowlers, while team b has 5 batsman and 6 bowlers\"\n",
    "\n",
    "#remove numbers by using regex\n",
    "output = re.sub(r\"\\d+\", \"\", input_str)\n",
    "\n",
    "#print the sentence after removal of numbers\n",
    "print(\"remove numbers :\", output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result after removing punctuation : Sentence having string with Punctuation\n"
     ]
    }
   ],
   "source": [
    "# Removing accent, punctuations marks, and other diacritics\n",
    "\n",
    "#Load the regex package and string package\n",
    "import re, string\n",
    "\n",
    "#define user input\n",
    "input_str = \"Sentence. having. string with. Punctuation?\"\n",
    "\n",
    "#remove punctuation\n",
    "result = re.sub('[%s]' % re.escape(string.punctuation), '', input_str)\n",
    "\n",
    "#print the sentence after removal of punctuation\n",
    "print(\"result after removing punctuation :\", result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text : Stop words are the words that are filtered before and after processing of text.\n",
      "-------------------------------------------------------------------------------------\n",
      "remove stopwords : ['Stop', 'words', 'words', 'filtered', 'processing', 'text', '.']\n",
      "-------------------------------------------------------------------------------------\n",
      "stopwords : {'are', 'before', 'of', 'the', 'that', 'after', 'and'}\n"
     ]
    }
   ],
   "source": [
    "#Load the stopwords package\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "#Load the word tokenizer package\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "#define the user input\n",
    "input_str = \"Stop words are the words that are filtered before and after processing of text.\"\n",
    "\n",
    "#crete object for stopwords\n",
    "stop_word = set(stopwords.words(\"english\"))\n",
    "#stop_word = stopwords.words(\"english\")\n",
    "\n",
    "#convert word into tokens\n",
    "token = word_tokenize(input_str)\n",
    "\n",
    "#remove stopwords from the list of tokens\n",
    "output = [i for i in token if not i in stop_word]\n",
    "stop_text = [i for i in token if  i in stop_word]\n",
    "\n",
    "#remove the stopwords and print the sentence\n",
    "print(\"original text :\", input_str )\n",
    "print(\"-------------------------------------------------------------------------------------\")\n",
    "print(\"remove stopwords :\", output)\n",
    "print(\"-------------------------------------------------------------------------------------\")\n",
    "print(\"stopwords :\", set(stop_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (NP He/PRP)\n",
      "  (VP reckons/VBZ)\n",
      "  (NP the/DT current/JJ account/NN deficit/NN)\n",
      "  (VP will/MD narrow/VB)\n",
      "  (PP to/TO)\n",
      "  (NP only/RB #/# 1.8/CD billion/CD)\n",
      "  (PP in/IN)\n",
      "  (NP September/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import conll2000\n",
    "chunked_sent = conll2000.chunked_sents()[10]\n",
    "print(chunked_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunked_sent.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('He', 'PRP', 'B-NP'), ('reckons', 'VBZ', 'B-VP'), ('the', 'DT', 'B-NP'), ('current', 'JJ', 'I-NP'), ('account', 'NN', 'I-NP'), ('deficit', 'NN', 'I-NP'), ('will', 'MD', 'B-VP'), ('narrow', 'VB', 'I-VP'), ('to', 'TO', 'B-PP'), ('only', 'RB', 'B-NP'), ('#', '#', 'I-NP'), ('1.8', 'CD', 'I-NP'), ('billion', 'CD', 'I-NP'), ('in', 'IN', 'B-PP'), ('September', 'NNP', 'B-NP'), ('.', '.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "# inside outside begning format\n",
    "from nltk.chunk import tree2conlltags, conlltags2tree\n",
    "iob_tageed = tree2conlltags(chunked_sent)\n",
    "print(iob_tageed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166433\n",
      "10948\n"
     ]
    }
   ],
   "source": [
    "print(len(conll2000.chunked_words()))\n",
    "print(len(conll2000.chunked_sents()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Chinking* is basicaaly identifying chunk of information, you would like to remove from chunks identified by chunk identifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = \"\"\"We are going to chink this sentence to remove all nouns. All of the other\n",
    "    words will be there. Except for the nouns\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('We', 'PRP'), ('are', 'VBP'), ('going', 'VBG'), ('to', 'TO'), ('chink', 'VB'), ('this', 'DT'), ('sentence', 'NN'), ('to', 'TO'), ('remove', 'VB'), ('all', 'DT'), ('nouns', 'NNS'), ('.', '.'), ('All', 'DT'), ('of', 'IN'), ('the', 'DT'), ('other', 'JJ'), ('words', 'NNS'), ('will', 'MD'), ('be', 'VB'), ('there', 'RB'), ('.', '.'), ('Except', 'IN'), ('for', 'IN'), ('the', 'DT'), ('nouns', 'NNS')]\n"
     ]
    }
   ],
   "source": [
    "print(nltk.pos_tag(word_tokenize(sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunkRule = r\"Chunk: {<.*>+}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunkParser = nltk.RegexpParser(chunkRule)\n",
    "chunkSent = chunkParser.parse(nltk.pos_tag(word_tokenize(sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunkSent.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{}\n",
    "}{"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunkRule = r\"\"\"Chunk: {<.*>+}\n",
    "                        }<NN.?|NNS|NNP|NNPS>+{\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunkParser = nltk.RegexpParser(chunkRule)\n",
    "chunkSent = chunkParser.parse(nltk.pos_tag(word_tokenize(sent)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunkSent.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "sent = \"We are going to chink this sentence to remoe all verbs\"\n",
    "chunkRule = r\"\"\"Chunk: {<.*>+}\n",
    "                        }<VB.?|VBP|VBG>+{\"\"\"\n",
    "chunkParser = nltk.RegexpParser(chunkRule)\n",
    "chunkSent = chunkParser.parse(nltk.pos_tag(word_tokenize(sent)))\n",
    "chunkSent.draw()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
