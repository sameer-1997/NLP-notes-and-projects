{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from math import *\n",
    "import operator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option(\"display.max_columns\", 10000)\n",
    "\n",
    "# import plotting libraries\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "from matplotlib import style\n",
    "%matplotlib inline \n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\", color_codes=True)\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "# For text processing\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.tokenize import sent_tokenize \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"It was the best of times it it  it it\" ,\n",
    "    \"it was the worst of times\",\n",
    "    \"it was the age of wisdom and lots of wisdom\",\n",
    "    \"it was the age of foolishness\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the count vectorizer\n",
    "vect_cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train (Bow) \n",
    "vect_cv.fit(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['age', 'and', 'best', 'foolishness', 'it', 'lots', 'of', 'the', 'times', 'was', 'wisdom', 'worst']\n",
      "Vocabulary size: 12\n"
     ]
    }
   ],
   "source": [
    "# get all the features/tokens\n",
    "feature_names = vect_cv.get_feature_names()\n",
    "print(feature_names)\n",
    "\n",
    "# get count of tokens\n",
    "print(\"Vocabulary size: {}\".format(len(vect_cv.vocabulary_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position Token\n",
      "       0 age\n",
      "       1 and\n",
      "       2 best\n",
      "       3 foolishness\n",
      "       4 it\n",
      "       5 lots\n",
      "       6 of\n",
      "       7 the\n",
      "       8 times\n",
      "       9 was\n",
      "      10 wisdom\n",
      "      11 worst\n"
     ]
    }
   ],
   "source": [
    "# print vocab in sorted manner\n",
    "def get_key(val): \n",
    "    for key, value in vect_cv.vocabulary_.items(): \n",
    "         if val == value: \n",
    "            return key \n",
    "\n",
    "print('Position', 'Token')\n",
    "for v in sorted(vect_cv.vocabulary_.values()) :  \n",
    "     print('{:8d} {}'.format(v, get_key(v) )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dtm\n",
    "X_train_cv_dtm = vect_cv.transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<4x12 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 26 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cv_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 5, 0, 1, 1, 1, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1],\n",
       "       [1, 1, 0, 0, 1, 1, 2, 1, 0, 1, 2, 0],\n",
       "       [1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cv_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform new test samples\n",
    "test_texts = [\n",
    "    \"Pollution is very bad for health\" ,\n",
    "    \"Govt not very keen on pollution control measures\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dtm\n",
    "test_dtm = vect_cv.transform(test_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dtm.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 2 (binary representation)\n",
    "\n",
    "- note the default lowercasing of the tokens\n",
    "- stop words are not removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"I love apples. Apples are good for health. An apple a day keeps the doctor away\",\n",
    "    \"Play football. It is very exciting. Football is played every where\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the count vectorizer\n",
    "vect_cv = CountVectorizer(binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=True, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train (Bow) \n",
    "vect_cv.fit(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['an', 'apple', 'apples', 'are', 'away', 'day', 'doctor', 'every', 'exciting', 'football', 'for', 'good', 'health', 'is', 'it', 'keeps', 'love', 'play', 'played', 'the', 'very', 'where']\n",
      "Vocabulary size: 22\n"
     ]
    }
   ],
   "source": [
    "# get all the features/tokens\n",
    "feature_names = vect_cv.get_feature_names()\n",
    "print(feature_names)\n",
    "\n",
    "# get count of tokens\n",
    "print(\"Vocabulary size: {}\".format(len(vect_cv.vocabulary_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position Token\n",
      "       0 an\n",
      "       1 apple\n",
      "       2 apples\n",
      "       3 are\n",
      "       4 away\n",
      "       5 day\n",
      "       6 doctor\n",
      "       7 every\n",
      "       8 exciting\n",
      "       9 football\n",
      "      10 for\n",
      "      11 good\n",
      "      12 health\n",
      "      13 is\n",
      "      14 it\n",
      "      15 keeps\n",
      "      16 love\n",
      "      17 play\n",
      "      18 played\n",
      "      19 the\n",
      "      20 very\n",
      "      21 where\n"
     ]
    }
   ],
   "source": [
    "# print vocab in sorted manner\n",
    "def get_key(val): \n",
    "    for key, value in vect_cv.vocabulary_.items(): \n",
    "         if val == value: \n",
    "            return key \n",
    "\n",
    "print('Position', 'Token')\n",
    "for v in sorted(vect_cv.vocabulary_.values()) :  \n",
    "     print('{:8d} {}'.format(v, get_key(v) )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dtm\n",
    "X_train_cv_dtm = vect_cv.transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2x22 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 22 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cv_dtm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1]],\n",
       "      dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cv_dtm.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3 - (max_df and min_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_list = ['3-idiots', 'Joker', 'Petta', 'Kaappaan', 'Kabir', 'Drishtikone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3-idiots Kaappaan Joker Petta',\n",
       " '3-idiots Joker Petta Drishtikone',\n",
       " 'Petta Kaappaan Kaappaan Kabir',\n",
       " 'Kabir Kaappaan 3-idiots Kaappaan',\n",
       " 'Petta 3-idiots Kabir Kaappaan',\n",
       " 'Kabir 3-idiots Drishtikone Kaappaan',\n",
       " 'Joker Kabir Kabir Kabir',\n",
       " 'Petta Drishtikone Joker Kabir',\n",
       " 'Petta Joker Kaappaan 3-idiots',\n",
       " '3-idiots Drishtikone Kabir Kabir']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies = []\n",
    "np.random.seed(100)\n",
    "\n",
    "for i in range(10):\n",
    "    movie_names_arrray = random.choices(movie_list, k=4)\n",
    "    movie_names_str    = ' '.join(movie_names_arrray)\n",
    "    \n",
    "    movies.append(movie_names_str)\n",
    "    \n",
    "#movies = np.array(movies)\n",
    "movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the count vectorizer\n",
    "vect_cv = CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train (Bow) \n",
    "vect_cv.fit(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['drishtikone', 'idiots', 'joker', 'kaappaan', 'kabir', 'petta']\n",
      "Vocabulary size: 6\n"
     ]
    }
   ],
   "source": [
    "# get all the features/tokens\n",
    "feature_names = vect_cv.get_feature_names()\n",
    "print(feature_names)\n",
    "\n",
    "# get count of tokens\n",
    "print(\"Vocabulary size: {}\".format(len(vect_cv.vocabulary_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position Token\n",
      "       0 drishtikone\n",
      "       1 idiots\n",
      "       2 joker\n",
      "       3 kaappaan\n",
      "       4 kabir\n",
      "       5 petta\n"
     ]
    }
   ],
   "source": [
    "# print vocab in sorted manner\n",
    "def get_key(val): \n",
    "    for key, value in vect_cv.vocabulary_.items(): \n",
    "         if val == value: \n",
    "            return key \n",
    "\n",
    "print('Position', 'Token')\n",
    "for v in sorted(vect_cv.vocabulary_.values()) :  \n",
    "     print('{:8d} {}'.format(v, get_key(v) )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dtm\n",
    "X_train_cv_dtm = vect_cv.transform(movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 1],\n",
       "       [1, 1, 1, 0, 0, 1],\n",
       "       [0, 0, 0, 2, 1, 1],\n",
       "       [0, 1, 0, 2, 1, 0],\n",
       "       [0, 1, 0, 1, 1, 1],\n",
       "       [1, 1, 0, 1, 1, 0],\n",
       "       [0, 0, 1, 0, 3, 0],\n",
       "       [1, 0, 1, 0, 1, 1],\n",
       "       [0, 1, 1, 1, 0, 1],\n",
       "       [1, 1, 0, 0, 2, 0]], dtype=int64)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_cv_dtm.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_docs = X_train_cv_dtm.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idiots          count =   7, DF =   70.00\n",
      "kaappaan        count =   6, DF =   60.00\n",
      "joker           count =   5, DF =   50.00\n",
      "petta           count =   6, DF =   60.00\n",
      "drishtikone     count =   4, DF =   40.00\n",
      "kabir           count =   7, DF =   70.00\n"
     ]
    }
   ],
   "source": [
    "# count how many times a token appears in the corpus\n",
    "for token in vect_cv.vocabulary_.keys():\n",
    "    \n",
    "    counter = 0\n",
    "    \n",
    "    # read each document\n",
    "    for doc in movies:\n",
    "\n",
    "        # check if the token appears in the document, if YES, increment the counter\n",
    "        if re.search(token, str(doc), re.IGNORECASE):\n",
    "            counter +=1\n",
    "    \n",
    "    print('{:15s} count = {:3d}, DF = {:7.2f}'.format(token, counter, (counter/number_docs)*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Max_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['drishtikone', 'joker', 'kaappaan', 'petta']\n",
      "Vocabulary size: 4\n",
      "Position Token\n",
      "       0 drishtikone\n",
      "       1 joker\n",
      "       2 kaappaan\n",
      "       3 petta\n"
     ]
    }
   ],
   "source": [
    "# instantiate the count vectorizer\n",
    "vect_cv = CountVectorizer(max_df=.65)\n",
    "\n",
    "# train (Bow) \n",
    "vect_cv.fit(movies)\n",
    "\n",
    "# get all the features/tokens\n",
    "feature_names = vect_cv.get_feature_names()\n",
    "print(feature_names)\n",
    "\n",
    "# get count of tokens\n",
    "print(\"Vocabulary size: {}\".format(len(vect_cv.vocabulary_)))\n",
    "\n",
    "# print vocab in sorted manner\n",
    "def get_key(val): \n",
    "    for key, value in vect_cv.vocabulary_.items(): \n",
    "         if val == value: \n",
    "            return key \n",
    "\n",
    "print('Position', 'Token')\n",
    "for v in sorted(vect_cv.vocabulary_.values()) :  \n",
    "     print('{:8d} {}'.format(v, get_key(v) )) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 0, 1],\n",
       "       [0, 2, 0, 1, 0],\n",
       "       [1, 0, 0, 2, 0],\n",
       "       [0, 1, 1, 0, 0],\n",
       "       [0, 1, 0, 1, 1],\n",
       "       [0, 0, 2, 0, 1],\n",
       "       [0, 0, 1, 1, 1],\n",
       "       [0, 0, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 1],\n",
       "       [1, 0, 0, 2, 0]], dtype=int64)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare dtm\n",
    "X_train_cv_dtm = vect_cv.transform(movies)\n",
    "\n",
    "X_train_cv_dtm.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### min_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['drishtikone', 'idiots', 'joker', 'kaappaan', 'kabir', 'petta']\n",
      "Vocabulary size: 6\n",
      "Position Token\n",
      "       0 drishtikone\n",
      "       1 idiots\n",
      "       2 joker\n",
      "       3 kaappaan\n",
      "       4 kabir\n",
      "       5 petta\n"
     ]
    }
   ],
   "source": [
    "# instantiate the count vectorizer\n",
    "vect_cv = CountVectorizer(max_df=.75, min_df=.30)\n",
    "\n",
    "# train (Bow) \n",
    "vect_cv.fit(movies)\n",
    "\n",
    "# get all the features/tokens\n",
    "feature_names = vect_cv.get_feature_names()\n",
    "print(feature_names)\n",
    "\n",
    "# get count of tokens\n",
    "print(\"Vocabulary size: {}\".format(len(vect_cv.vocabulary_)))\n",
    "\n",
    "# print vocab in sorted manner\n",
    "def get_key(val): \n",
    "    for key, value in vect_cv.vocabulary_.items(): \n",
    "         if val == value: \n",
    "            return key \n",
    "\n",
    "print('Position', 'Token')\n",
    "for v in sorted(vect_cv.vocabulary_.values()) :  \n",
    "     print('{:8d} {}'.format(v, get_key(v) )) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 3 - ngram_range or n-gram\n",
    "\n",
    "#### What is an n-gram?\n",
    "\n",
    "An n-gram is a contiguous sequence of n __items__ from a given sequence of text. \n",
    "\n",
    "Given a sentence, s, we can construct a list of n-grams from s by finding pairs of words that occur next to each other. \n",
    "\n",
    "Here an __item__ can be a character, a word or a sentence and N can be any integer. \n",
    "\n",
    "- When N is 2, we call the sequence a bigram.\n",
    "- Similarly, a sequence of 3 items is called a trigram, and so on.\n",
    "\n",
    "For example, given the sentence “I am Rajat” you can construct bigrams (n-grams of length 2) by finding consecutive pairs of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### word grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"I studied DS/ML/DL at IISc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'studied', 'DS/ML/DL', 'at', 'IISc']"
      ]
     },
     "execution_count": 207,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = s.split(\" \")\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'studied'), ('studied', 'DS/ML/DL'), ('DS/ML/DL', 'at'), ('at', 'IISc')]"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = [(tokens[i],tokens[i+1]) for i in range(0, len(tokens)-1)]\n",
    "bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', 'studied', 'DS/ML/DL'),\n",
       " ('studied', 'DS/ML/DL', 'at'),\n",
       " ('DS/ML/DL', 'at', 'IISc')]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams = [(tokens[i],tokens[i+1],tokens[i+2]) for i in range(0, len(tokens)-2)]\n",
    "trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### character grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [ch for ch in s]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', ' '),\n",
       " (' ', 's'),\n",
       " ('s', 't'),\n",
       " ('t', 'u'),\n",
       " ('u', 'd'),\n",
       " ('d', 'i'),\n",
       " ('i', 'e'),\n",
       " ('e', 'd'),\n",
       " ('d', ' '),\n",
       " (' ', 'D'),\n",
       " ('D', 'S'),\n",
       " ('S', '/'),\n",
       " ('/', 'M'),\n",
       " ('M', 'L'),\n",
       " ('L', '/'),\n",
       " ('/', 'D'),\n",
       " ('D', 'L'),\n",
       " ('L', ' '),\n",
       " (' ', 'a'),\n",
       " ('a', 't'),\n",
       " ('t', ' '),\n",
       " (' ', 'I'),\n",
       " ('I', 'I'),\n",
       " ('I', 'S'),\n",
       " ('S', 'c')]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = [(tokens[i],tokens[i+1]) for i in range(0, len(tokens)-1)]\n",
    "bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', ' ', 's'),\n",
       " (' ', 's', 't'),\n",
       " ('s', 't', 'u'),\n",
       " ('t', 'u', 'd'),\n",
       " ('u', 'd', 'i'),\n",
       " ('d', 'i', 'e'),\n",
       " ('i', 'e', 'd'),\n",
       " ('e', 'd', ' '),\n",
       " ('d', ' ', 'D'),\n",
       " (' ', 'D', 'S'),\n",
       " ('D', 'S', '/'),\n",
       " ('S', '/', 'M'),\n",
       " ('/', 'M', 'L'),\n",
       " ('M', 'L', '/'),\n",
       " ('L', '/', 'D'),\n",
       " ('/', 'D', 'L'),\n",
       " ('D', 'L', ' '),\n",
       " ('L', ' ', 'a'),\n",
       " (' ', 'a', 't'),\n",
       " ('a', 't', ' '),\n",
       " ('t', ' ', 'I'),\n",
       " (' ', 'I', 'I'),\n",
       " ('I', 'I', 'S'),\n",
       " ('I', 'S', 'c')]"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams = [(tokens[i],tokens[i+1],tokens[i+2]) for i in range(0, len(tokens)-2)]\n",
    "trigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sentence level n-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = ['I studied DS/ML/DL at IISc',\n",
    "          'IISc is a great place to study',\n",
    "          'IISc courses are cheap too',\n",
    "          'course duration is 5 months'\n",
    "         ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I studied DS/ML/DL at IISc', 'IISc is a great place to study'),\n",
       " ('IISc is a great place to study', 'IISc courses are cheap too'),\n",
       " ('IISc courses are cheap too', 'course duration is 5 months')]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigrams = [(tokens[i],tokens[i+1]) for i in range(0, len(tokens)-1)]\n",
    "bigrams\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Characters N-Grams Model - with a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "\n",
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the Beautifulsoup4 library to parse the data from Wikipedia. Furthermore, Python's regex library, re, will be used for some preprocessing tasks on the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_html = urllib.request.urlopen('https://en.wikipedia.org/wiki/Tennis')\n",
    "raw_html = raw_html.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_html       = bs.BeautifulSoup(raw_html, 'lxml')\n",
    "article_paragraphs = article_html.find_all('p')\n",
    "\n",
    "article_text = ''\n",
    "\n",
    "for para in article_paragraphs:\n",
    "    article_text += para.text\n",
    "\n",
    "article_text = article_text.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we remove everything from our dataset __except letters, periods, and spaces_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "article_text = re.sub(r'[^A-Za-z. ]', '', article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57445"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tennis is a racket sport that can be played individually against a single opponent singles or between two teams of two players each doubles. each player uses a tennis racket that is strung with cord to strike a hollow rubber ball covered with felt over or around a net and into the opponents court. the object of the game is to maneuver the ball in such a way that the opponent is not able to play a valid return. the player who is unable to return the ball will not gain a point while the opposite p'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "article_text[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create trigrams\n",
    "\n",
    "create a dictionary ngrams. The keys of this dictionary will be the character trigrams in our corpus and the values will be the characters that occur next to the trigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a small example text\n",
    "article_text = 'I studied Data Science at IISc in 2010'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams = {}\n",
    "chars  = 3\n",
    "\n",
    "for i in range(len(article_text) - chars):\n",
    "    \n",
    "    seq = article_text[i:i+chars]\n",
    "    #print(seq)\n",
    "    \n",
    "    if seq not in ngrams.keys():\n",
    "        ngrams[seq] = []\n",
    "        \n",
    "    ngrams[seq].append(article_text[i+chars])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I s ['t']\n",
      " st ['u']\n",
      "stu ['d']\n",
      "tud ['i']\n",
      "udi ['e']\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "for k, v in ngrams.items():\n",
    "    \n",
    "    if i> 5: \n",
    "        break\n",
    "        \n",
    "    print(k, v)\n",
    "\n",
    "    i +=1 \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see __trigrams as keys__, and the corresponding characters, which occur after the trigrams in the text, as __values__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predict "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "studied Data Sc\n"
     ]
    }
   ],
   "source": [
    "#curr_sequence = article_text[0:chars]\n",
    "curr_sequence = 'stu'\n",
    "output        = curr_sequence\n",
    "\n",
    "for i in range(12):\n",
    "    \n",
    "    if curr_sequence not in ngrams.keys():\n",
    "        break\n",
    "        \n",
    "    possible_chars = ngrams[curr_sequence]\n",
    "    next_char      = possible_chars[random.randrange(len(possible_chars))]\n",
    "    \n",
    "    output        += next_char\n",
    "    \n",
    "    curr_sequence = output[ len(output) - chars:len(output) ]\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words N-Grams Model\n",
    "\n",
    "first create a dictionary that contains word trigrams as keys and the list of words that occur after the trigrams as values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a small example text\n",
    "article_text = 'I studied Data Science at IISc in 2010'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I studied Data\n",
      "studied Data Science\n",
      "Data Science at\n",
      "Science at IISc\n",
      "at IISc in\n"
     ]
    }
   ],
   "source": [
    "ngrams = {}\n",
    "words  = 3\n",
    "\n",
    "words_tokens = nltk.word_tokenize(article_text)\n",
    "\n",
    "for i in range(len(words_tokens) - words):\n",
    "    \n",
    "    seq = ' '.join(words_tokens[i:i+words])\n",
    "    \n",
    "    print(seq)\n",
    "    \n",
    "    if  seq not in ngrams.keys():\n",
    "        ngrams[seq] = []\n",
    "        \n",
    "    ngrams[seq].append(words_tokens[i+words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'I studied Data': ['Science'],\n",
       " 'studied Data Science': ['at'],\n",
       " 'Data Science at': ['IISc'],\n",
       " 'Science at IISc': ['in'],\n",
       " 'at IISc in': ['2010']}"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Science at IISc in 2010\n"
     ]
    }
   ],
   "source": [
    "#curr_sequence = ' '.join(words_tokens[0:words])\n",
    "curr_sequence = 'Science at IISc'\n",
    "\n",
    "output        = curr_sequence\n",
    "\n",
    "for i in range(50):\n",
    "    if curr_sequence not in ngrams.keys():\n",
    "        break\n",
    "        \n",
    "    possible_words = ngrams[curr_sequence]\n",
    "    next_word      = possible_words[random.randrange(len(possible_words))]\n",
    "    \n",
    "    output        += ' ' + next_word\n",
    "    \n",
    "    seq_words = nltk.word_tokenize(output)\n",
    "    curr_sequence = ' '.join(seq_words[len(seq_words)-words:len(seq_words)])\n",
    "\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-grams Using NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = \"Natural-language processing (NLP) is an area of computer science \" \\\n",
    "    \"and artificial intelligence concerned with the interactions \" \\\n",
    "    \"between computers and human (natural) languages. !!!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Natural-language processing (NLP) is an area of computer science and artificial intelligence concerned with the interactions between computers and human (natural) languages. !!!'"
      ]
     },
     "execution_count": 327,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = s.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'natural language processing  nlp  is an area of computer science and artificial intelligence concerned with the interactions between computers and human  natural  languages     '"
      ]
     },
     "execution_count": 330,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = re.sub(r'[^a-zA-Z0-9\\s]', ' ', s)\n",
    "s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['natural',\n",
       " 'language',\n",
       " 'processing',\n",
       " 'nlp',\n",
       " 'is',\n",
       " 'an',\n",
       " 'area',\n",
       " 'of',\n",
       " 'computer',\n",
       " 'science',\n",
       " 'and',\n",
       " 'artificial',\n",
       " 'intelligence',\n",
       " 'concerned',\n",
       " 'with',\n",
       " 'the',\n",
       " 'interactions',\n",
       " 'between',\n",
       " 'computers',\n",
       " 'and',\n",
       " 'human',\n",
       " 'natural',\n",
       " 'languages']"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [token for token in s.split(\" \") if token != \"\"]\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('natural', 'language', 'processing', 'nlp', 'is'),\n",
       " ('language', 'processing', 'nlp', 'is', 'an'),\n",
       " ('processing', 'nlp', 'is', 'an', 'area'),\n",
       " ('nlp', 'is', 'an', 'area', 'of'),\n",
       " ('is', 'an', 'area', 'of', 'computer'),\n",
       " ('an', 'area', 'of', 'computer', 'science'),\n",
       " ('area', 'of', 'computer', 'science', 'and'),\n",
       " ('of', 'computer', 'science', 'and', 'artificial'),\n",
       " ('computer', 'science', 'and', 'artificial', 'intelligence'),\n",
       " ('science', 'and', 'artificial', 'intelligence', 'concerned'),\n",
       " ('and', 'artificial', 'intelligence', 'concerned', 'with'),\n",
       " ('artificial', 'intelligence', 'concerned', 'with', 'the'),\n",
       " ('intelligence', 'concerned', 'with', 'the', 'interactions'),\n",
       " ('concerned', 'with', 'the', 'interactions', 'between'),\n",
       " ('with', 'the', 'interactions', 'between', 'computers'),\n",
       " ('the', 'interactions', 'between', 'computers', 'and'),\n",
       " ('interactions', 'between', 'computers', 'and', 'human'),\n",
       " ('between', 'computers', 'and', 'human', 'natural'),\n",
       " ('computers', 'and', 'human', 'natural', 'languages')]"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = list(ngrams(tokens, 5))\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-grams in vectors for supervised learning problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    \"Penny bought bright blue fishes. !! ) %$&#&#**#*\",\n",
    "    \"Penny bought bright blue and orange fish.\",\n",
    "    \"The cat ate a fish at the store.\",\n",
    "    \"Penny went to the store. Penny ate a bug. Penny saw a fish fish .\",\n",
    "    \"It meowed once at the bug, it is still meowing at the bug and the fish\",\n",
    "    \"The cat is at the fish store. The cat is orange. The cat is meowing at the fish.\",\n",
    "    \"Penny is a fish\",\n",
    "    \"lets take this sentence for example\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N-grams (sets of consecutive words) N=2\n",
    "# instantiate the count vectorizer\n",
    "vect_cv = CountVectorizer(ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 347,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train (Bow) \n",
    "vect_cv.fit(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 78\n",
      "['and', 'and orange', 'and the', 'at', 'at the', 'ate', 'ate bug', 'ate fish', 'blue', 'blue and', 'blue fishes', 'bought', 'bought bright', 'bright', 'bright blue', 'bug', 'bug and', 'bug it', 'bug penny', 'cat', 'cat ate', 'cat is', 'example', 'fish', 'fish at', 'fish fish', 'fish store', 'fishes', 'for', 'for example', 'is', 'is at', 'is fish', 'is meowing', 'is orange', 'is still', 'it', 'it is', 'it meowed', 'lets', 'lets take', 'meowed', 'meowed once', 'meowing', 'meowing at', 'once', 'once at', 'orange', 'orange fish', 'orange the', 'penny', 'penny ate', 'penny bought', 'penny is', 'penny saw', 'penny went', 'saw', 'saw fish', 'sentence', 'sentence for', 'still', 'still meowing', 'store', 'store penny', 'store the', 'take', 'take this', 'the', 'the bug', 'the cat', 'the fish', 'the store', 'this', 'this sentence', 'to', 'to the', 'went', 'went to']\n",
      "Vocabulary content:\n",
      " {'penny': 50, 'bought': 11, 'bright': 13, 'blue': 8, 'fishes': 27, 'penny bought': 52, 'bought bright': 12, 'bright blue': 14, 'blue fishes': 10, 'and': 0, 'orange': 47, 'fish': 23, 'blue and': 9, 'and orange': 1, 'orange fish': 48, 'the': 67, 'cat': 19, 'ate': 5, 'at': 3, 'store': 62, 'the cat': 69, 'cat ate': 20, 'ate fish': 7, 'fish at': 24, 'at the': 4, 'the store': 71, 'went': 76, 'to': 74, 'bug': 15, 'saw': 56, 'penny went': 55, 'went to': 77, 'to the': 75, 'store penny': 63, 'penny ate': 51, 'ate bug': 6, 'bug penny': 18, 'penny saw': 54, 'saw fish': 57, 'fish fish': 25, 'it': 36, 'meowed': 41, 'once': 45, 'is': 30, 'still': 60, 'meowing': 43, 'it meowed': 38, 'meowed once': 42, 'once at': 46, 'the bug': 68, 'bug it': 17, 'it is': 37, 'is still': 35, 'still meowing': 61, 'meowing at': 44, 'bug and': 16, 'and the': 2, 'the fish': 70, 'cat is': 21, 'is at': 31, 'fish store': 26, 'store the': 64, 'is orange': 34, 'orange the': 49, 'is meowing': 33, 'penny is': 53, 'is fish': 32, 'lets': 39, 'take': 65, 'this': 72, 'sentence': 58, 'for': 28, 'example': 22, 'lets take': 40, 'take this': 66, 'this sentence': 73, 'sentence for': 59, 'for example': 29}\n"
     ]
    }
   ],
   "source": [
    "# get all the feature/token names\n",
    "print(\"Vocabulary size: {}\".format(len(vect_cv.vocabulary_)))\n",
    "\n",
    "feature_names = vect_cv.get_feature_names()\n",
    "print(feature_names)\n",
    "\n",
    "print(\"Vocabulary content:\\n {}\".format(vect_cv.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dtm\n",
    "X_train_cv_dtm = vect_cv.transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>and</th>\n",
       "      <th>and orange</th>\n",
       "      <th>and the</th>\n",
       "      <th>at</th>\n",
       "      <th>at the</th>\n",
       "      <th>ate</th>\n",
       "      <th>ate bug</th>\n",
       "      <th>ate fish</th>\n",
       "      <th>blue</th>\n",
       "      <th>blue and</th>\n",
       "      <th>blue fishes</th>\n",
       "      <th>bought</th>\n",
       "      <th>bought bright</th>\n",
       "      <th>bright</th>\n",
       "      <th>bright blue</th>\n",
       "      <th>bug</th>\n",
       "      <th>bug and</th>\n",
       "      <th>bug it</th>\n",
       "      <th>bug penny</th>\n",
       "      <th>cat</th>\n",
       "      <th>cat ate</th>\n",
       "      <th>cat is</th>\n",
       "      <th>example</th>\n",
       "      <th>fish</th>\n",
       "      <th>fish at</th>\n",
       "      <th>fish fish</th>\n",
       "      <th>fish store</th>\n",
       "      <th>fishes</th>\n",
       "      <th>for</th>\n",
       "      <th>for example</th>\n",
       "      <th>is</th>\n",
       "      <th>is at</th>\n",
       "      <th>is fish</th>\n",
       "      <th>is meowing</th>\n",
       "      <th>is orange</th>\n",
       "      <th>is still</th>\n",
       "      <th>it</th>\n",
       "      <th>it is</th>\n",
       "      <th>it meowed</th>\n",
       "      <th>lets</th>\n",
       "      <th>lets take</th>\n",
       "      <th>meowed</th>\n",
       "      <th>meowed once</th>\n",
       "      <th>meowing</th>\n",
       "      <th>meowing at</th>\n",
       "      <th>once</th>\n",
       "      <th>once at</th>\n",
       "      <th>orange</th>\n",
       "      <th>orange fish</th>\n",
       "      <th>orange the</th>\n",
       "      <th>penny</th>\n",
       "      <th>penny ate</th>\n",
       "      <th>penny bought</th>\n",
       "      <th>penny is</th>\n",
       "      <th>penny saw</th>\n",
       "      <th>penny went</th>\n",
       "      <th>saw</th>\n",
       "      <th>saw fish</th>\n",
       "      <th>sentence</th>\n",
       "      <th>sentence for</th>\n",
       "      <th>still</th>\n",
       "      <th>still meowing</th>\n",
       "      <th>store</th>\n",
       "      <th>store penny</th>\n",
       "      <th>store the</th>\n",
       "      <th>take</th>\n",
       "      <th>take this</th>\n",
       "      <th>the</th>\n",
       "      <th>the bug</th>\n",
       "      <th>the cat</th>\n",
       "      <th>the fish</th>\n",
       "      <th>the store</th>\n",
       "      <th>this</th>\n",
       "      <th>this sentence</th>\n",
       "      <th>to</th>\n",
       "      <th>to the</th>\n",
       "      <th>went</th>\n",
       "      <th>went to</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   and  and orange  and the  at  at the  ate  ate bug  ate fish  blue  \\\n",
       "0    0           0        0   0       0    0        0         0     1   \n",
       "1    1           1        0   0       0    0        0         0     1   \n",
       "2    0           0        0   1       1    1        0         1     0   \n",
       "3    0           0        0   0       0    1        1         0     0   \n",
       "4    1           0        1   2       2    0        0         0     0   \n",
       "5    0           0        0   2       2    0        0         0     0   \n",
       "6    0           0        0   0       0    0        0         0     0   \n",
       "7    0           0        0   0       0    0        0         0     0   \n",
       "\n",
       "   blue and  blue fishes  bought  bought bright  bright  bright blue  bug  \\\n",
       "0         0            1       1              1       1            1    0   \n",
       "1         1            0       1              1       1            1    0   \n",
       "2         0            0       0              0       0            0    0   \n",
       "3         0            0       0              0       0            0    1   \n",
       "4         0            0       0              0       0            0    2   \n",
       "5         0            0       0              0       0            0    0   \n",
       "6         0            0       0              0       0            0    0   \n",
       "7         0            0       0              0       0            0    0   \n",
       "\n",
       "   bug and  bug it  bug penny  cat  cat ate  cat is  example  fish  fish at  \\\n",
       "0        0       0          0    0        0       0        0     0        0   \n",
       "1        0       0          0    0        0       0        0     1        0   \n",
       "2        0       0          0    1        1       0        0     1        1   \n",
       "3        0       0          1    0        0       0        0     2        0   \n",
       "4        1       1          0    0        0       0        0     1        0   \n",
       "5        0       0          0    3        0       3        0     2        0   \n",
       "6        0       0          0    0        0       0        0     1        0   \n",
       "7        0       0          0    0        0       0        1     0        0   \n",
       "\n",
       "   fish fish  fish store  fishes  for  for example  is  is at  is fish  \\\n",
       "0          0           0       1    0            0   0      0        0   \n",
       "1          0           0       0    0            0   0      0        0   \n",
       "2          0           0       0    0            0   0      0        0   \n",
       "3          1           0       0    0            0   0      0        0   \n",
       "4          0           0       0    0            0   1      0        0   \n",
       "5          0           1       0    0            0   3      1        0   \n",
       "6          0           0       0    0            0   1      0        1   \n",
       "7          0           0       0    1            1   0      0        0   \n",
       "\n",
       "   is meowing  is orange  is still  it  it is  it meowed  lets  lets take  \\\n",
       "0           0          0         0   0      0          0     0          0   \n",
       "1           0          0         0   0      0          0     0          0   \n",
       "2           0          0         0   0      0          0     0          0   \n",
       "3           0          0         0   0      0          0     0          0   \n",
       "4           0          0         1   2      1          1     0          0   \n",
       "5           1          1         0   0      0          0     0          0   \n",
       "6           0          0         0   0      0          0     0          0   \n",
       "7           0          0         0   0      0          0     1          1   \n",
       "\n",
       "   meowed  meowed once  meowing  meowing at  once  once at  orange  \\\n",
       "0       0            0        0           0     0        0       0   \n",
       "1       0            0        0           0     0        0       1   \n",
       "2       0            0        0           0     0        0       0   \n",
       "3       0            0        0           0     0        0       0   \n",
       "4       1            1        1           1     1        1       0   \n",
       "5       0            0        1           1     0        0       1   \n",
       "6       0            0        0           0     0        0       0   \n",
       "7       0            0        0           0     0        0       0   \n",
       "\n",
       "   orange fish  orange the  penny  penny ate  penny bought  penny is  \\\n",
       "0            0           0      1          0             1         0   \n",
       "1            1           0      1          0             1         0   \n",
       "2            0           0      0          0             0         0   \n",
       "3            0           0      3          1             0         0   \n",
       "4            0           0      0          0             0         0   \n",
       "5            0           1      0          0             0         0   \n",
       "6            0           0      1          0             0         1   \n",
       "7            0           0      0          0             0         0   \n",
       "\n",
       "   penny saw  penny went  saw  saw fish  sentence  sentence for  still  \\\n",
       "0          0           0    0         0         0             0      0   \n",
       "1          0           0    0         0         0             0      0   \n",
       "2          0           0    0         0         0             0      0   \n",
       "3          1           1    1         1         0             0      0   \n",
       "4          0           0    0         0         0             0      1   \n",
       "5          0           0    0         0         0             0      0   \n",
       "6          0           0    0         0         0             0      0   \n",
       "7          0           0    0         0         1             1      0   \n",
       "\n",
       "   still meowing  store  store penny  store the  take  take this  the  \\\n",
       "0              0      0            0          0     0          0    0   \n",
       "1              0      0            0          0     0          0    0   \n",
       "2              0      1            0          0     0          0    2   \n",
       "3              0      1            1          0     0          0    1   \n",
       "4              1      0            0          0     0          0    3   \n",
       "5              0      1            0          1     0          0    5   \n",
       "6              0      0            0          0     0          0    0   \n",
       "7              0      0            0          0     1          1    0   \n",
       "\n",
       "   the bug  the cat  the fish  the store  this  this sentence  to  to the  \\\n",
       "0        0        0         0          0     0              0   0       0   \n",
       "1        0        0         0          0     0              0   0       0   \n",
       "2        0        1         0          1     0              0   0       0   \n",
       "3        0        0         0          1     0              0   1       1   \n",
       "4        2        0         1          0     0              0   0       0   \n",
       "5        0        3         2          0     0              0   0       0   \n",
       "6        0        0         0          0     0              0   0       0   \n",
       "7        0        0         0          0     1              1   0       0   \n",
       "\n",
       "   went  went to  \n",
       "0     0        0  \n",
       "1     0        0  \n",
       "2     0        0  \n",
       "3     1        1  \n",
       "4     0        0  \n",
       "5     0        0  \n",
       "6     0        0  \n",
       "7     0        0  "
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dataframe\n",
    "pd.DataFrame(X_train_cv_dtm.toarray(), columns=feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example - 4 (Stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the count vectorizer\n",
    "vect_cv = CountVectorizer(stop_words='english', max_features=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train (Bow) \n",
    "vect_cv.fit(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 13\n",
      "['apple', 'apples', 'away', 'day', 'doctor', 'exciting', 'football', 'good', 'health', 'keeps', 'love', 'play', 'played']\n",
      "Vocabulary content:\n",
      " {'love': 10, 'apples': 1, 'good': 7, 'health': 8, 'apple': 0, 'day': 3, 'keeps': 9, 'doctor': 4, 'away': 2, 'play': 11, 'football': 6, 'exciting': 5, 'played': 12}\n"
     ]
    }
   ],
   "source": [
    "# get all the feature/token names\n",
    "print(\"Vocabulary size: {}\".format(len(vect_cv.vocabulary_)))\n",
    "\n",
    "feature_names = vect_cv.get_feature_names()\n",
    "print(feature_names)\n",
    "\n",
    "print(\"Vocabulary content:\\n {}\".format(vect_cv.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "# notice the lack of stemming .. fish and fishes, meowed\tmeowing\n",
    "\n",
    "# CountVectorizer can \n",
    "# - lowercase letters, \n",
    "# - disregard punctuation and \n",
    "# - stopwords, \n",
    "\n",
    "# but it can't LEMMATIZE or STEM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fish\n",
      "fish\n",
      "meow\n",
      "meow\n"
     ]
    }
   ],
   "source": [
    "# create the stemmer object\n",
    "porter_stemmer = PorterStemmer()\n",
    "print(porter_stemmer.stem(\"fish\"))\n",
    "print(porter_stemmer.stem(\"fishes\"))\n",
    "print(porter_stemmer.stem(\"meowed\"))\n",
    "print(porter_stemmer.stem(\"meowing\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use NLTK's PorterStemmer\n",
    "def stemming_tokenizer(str_input):\n",
    "    words = re.sub(r\"[^A-Za-z0-9\\-]\", \" \", str_input).lower().split()\n",
    "    words = [porter_stemmer.stem(word) for word in words]\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the count vectorizer\n",
    "vect_cv = CountVectorizer(ngram_range=(1, 1), stop_words='english', tokenizer=stemming_tokenizer, max_features=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=<function stemming_tokenizer at 0x0000017CC5AE09D8>,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 358,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train (Bow) \n",
    "vect_cv.fit(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 18\n",
      "['ate', 'blue', 'bought', 'bright', 'bug', 'cat', 'exampl', 'fish', 'let', 'meow', 'onc', 'orang', 'penni', 'saw', 'sentenc', 'store', 'thi', 'went']\n",
      "Vocabulary content:\n",
      " {'penni': 12, 'bought': 2, 'bright': 3, 'blue': 1, 'fish': 7, 'orang': 11, 'cat': 5, 'ate': 0, 'store': 15, 'went': 17, 'bug': 4, 'saw': 13, 'meow': 9, 'onc': 10, 'let': 8, 'thi': 16, 'sentenc': 14, 'exampl': 6}\n"
     ]
    }
   ],
   "source": [
    "# get all the feature/token names\n",
    "print(\"Vocabulary size: {}\".format(len(vect_cv.vocabulary_)))\n",
    "\n",
    "feature_names = vect_cv.get_feature_names()\n",
    "print(feature_names)\n",
    "\n",
    "print(\"Vocabulary content:\\n {}\".format(vect_cv.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare dtm\n",
    "X_train_cv_dtm = vect_cv.transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ate</th>\n",
       "      <th>blue</th>\n",
       "      <th>bought</th>\n",
       "      <th>bright</th>\n",
       "      <th>bug</th>\n",
       "      <th>cat</th>\n",
       "      <th>exampl</th>\n",
       "      <th>fish</th>\n",
       "      <th>let</th>\n",
       "      <th>meow</th>\n",
       "      <th>onc</th>\n",
       "      <th>orang</th>\n",
       "      <th>penni</th>\n",
       "      <th>saw</th>\n",
       "      <th>sentenc</th>\n",
       "      <th>store</th>\n",
       "      <th>thi</th>\n",
       "      <th>went</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ate  blue  bought  bright  bug  cat  exampl  fish  let  meow  onc  orang  \\\n",
       "0    0     1       1       1    0    0       0     1    0     0    0      0   \n",
       "1    0     1       1       1    0    0       0     1    0     0    0      1   \n",
       "2    1     0       0       0    0    1       0     1    0     0    0      0   \n",
       "3    1     0       0       0    1    0       0     2    0     0    0      0   \n",
       "4    0     0       0       0    2    0       0     1    0     2    1      0   \n",
       "5    0     0       0       0    0    3       0     2    0     1    0      1   \n",
       "6    0     0       0       0    0    0       0     1    0     0    0      0   \n",
       "7    0     0       0       0    0    0       1     0    1     0    0      0   \n",
       "\n",
       "   penni  saw  sentenc  store  thi  went  \n",
       "0      1    0        0      0    0     0  \n",
       "1      1    0        0      0    0     0  \n",
       "2      0    0        0      1    0     0  \n",
       "3      3    1        0      1    0     1  \n",
       "4      0    0        0      0    0     0  \n",
       "5      0    0        0      1    0     0  \n",
       "6      1    0        0      0    0     0  \n",
       "7      0    0        1      0    1     0  "
      ]
     },
     "execution_count": 361,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create a dataframe\n",
    "pd.DataFrame(X_train_cv_dtm.toarray(), columns=feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
