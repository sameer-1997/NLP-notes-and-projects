{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from math import *\n",
    "import operator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option(\"display.max_columns\", 10000)\n",
    "import string\n",
    "\n",
    "# import plotting libraries\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "from matplotlib import style\n",
    "%matplotlib inline \n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\", color_codes=True)\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "# For text processing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.tokenize import sent_tokenize \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Importing Gensim\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Strings to Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "corpus of 9 documents, each consisting of only a single sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let’s tokenize the documents, remove common words (using a toy stoplist) as well as words that only appear once in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove common words and tokenize\n",
    "stoplist = set('for a of the and to in'.split())\n",
    "\n",
    "texts = [\n",
    "    [word for word in document.lower().split() if word not in stoplist]\n",
    "    for document in documents\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'machine', 'interface', 'lab', 'abc', 'computer', 'applications'],\n",
       " ['survey', 'user', 'opinion', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'management', 'system'],\n",
       " ['system', 'human', 'system', 'engineering', 'testing', 'eps'],\n",
       " ['relation', 'user', 'perceived', 'response', 'time', 'error', 'measurement'],\n",
       " ['generation', 'random', 'binary', 'unordered', 'trees'],\n",
       " ['intersection', 'graph', 'paths', 'trees'],\n",
       " ['graph', 'minors', 'iv', 'widths', 'trees', 'well', 'quasi', 'ordering'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove words that appear only once\n",
    "frequency = defaultdict(int)\n",
    "\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>,\n",
      "            {'abc': 1,\n",
      "             'applications': 1,\n",
      "             'binary': 1,\n",
      "             'computer': 2,\n",
      "             'engineering': 1,\n",
      "             'eps': 2,\n",
      "             'error': 1,\n",
      "             'generation': 1,\n",
      "             'graph': 3,\n",
      "             'human': 2,\n",
      "             'interface': 2,\n",
      "             'intersection': 1,\n",
      "             'iv': 1,\n",
      "             'lab': 1,\n",
      "             'machine': 1,\n",
      "             'management': 1,\n",
      "             'measurement': 1,\n",
      "             'minors': 2,\n",
      "             'opinion': 1,\n",
      "             'ordering': 1,\n",
      "             'paths': 1,\n",
      "             'perceived': 1,\n",
      "             'quasi': 1,\n",
      "             'random': 1,\n",
      "             'relation': 1,\n",
      "             'response': 2,\n",
      "             'survey': 2,\n",
      "             'system': 4,\n",
      "             'testing': 1,\n",
      "             'time': 2,\n",
      "             'trees': 3,\n",
      "             'unordered': 1,\n",
      "             'user': 3,\n",
      "             'well': 1,\n",
      "             'widths': 1})\n"
     ]
    }
   ],
   "source": [
    "pprint(frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    [token for token in text if frequency[token] > 1]\n",
    "    for text in texts\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'computer'],\n",
       " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'system'],\n",
       " ['system', 'human', 'system', 'eps'],\n",
       " ['user', 'response', 'time'],\n",
       " ['trees'],\n",
       " ['graph', 'trees'],\n",
       " ['graph', 'minors', 'trees'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "print(texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To convert documents to vectors, we’ll use a document representation called bag-of-words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(33 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...)\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "dictionary.save(r'C:\\Users\\Sky\\Desktop\\SimpliLearn/gensimdict.dict')  # store the dictionary, for future reference\n",
    "\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are twelve distinct words in the processed corpus, which means each document will be represented by twelve numbers (ie., by a 12-D vector).\n",
    "\n",
    "\n",
    "To see the mapping between words and their ids:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'computer': 0, 'human': 1, 'interface': 2, 'response': 3, 'survey': 4, 'system': 5, 'time': 6, 'user': 7, 'eps': 8, 'trees': 9, 'graph': 10, 'minors': 11}\n"
     ]
    }
   ],
   "source": [
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "convert tokenized documents to vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1)]\n"
     ]
    }
   ],
   "source": [
    "new_doc = \"Human computer interaction\"\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "print(new_vec)  # the word \"interaction\" does not appear in the dictionary and is ignored"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### doc2bow() \n",
    "- simply counts the number of occurrences of each distinct word, \n",
    "- converts the word to its integer word id and returns the result as a sparse vector. \n",
    "\n",
    "The sparse vector [(0, 1), (1, 1)] therefore reads: in the document “Human computer interaction”, the words computer (id 0) and human (id 1) appear once; the other ten dictionary words appear (implicitly) zero times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[(0, 1), (1, 1), (2, 1)], [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)], [(2, 1), (5, 1), (7, 1), (8, 1)], [(1, 1), (5, 2), (8, 1)], [(3, 1), (6, 1), (7, 1)], [(9, 1)], [(9, 1), (10, 1)], [(9, 1), (10, 1), (11, 1)], [(4, 1), (10, 1), (11, 1)]]\n"
     ]
    }
   ],
   "source": [
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "corpora.MmCorpus.serialize(r'C:\\Users\\Sky\\Desktop\\SimpliLearn/gensimbow.mm', corpus)  # store to disk, for later use\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Corpus Streaming – One Document at a Time\n",
    "\n",
    "Note that corpus above resides fully in memory, as a plain Python list. \n",
    "\n",
    "let’s assume there are millions of documents in the corpus. Storing all of them in RAM won’t do. \n",
    "\n",
    "Instead, let’s assume the documents are stored in a file on disk, one document per line. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to create a Dictionary from a list of sentences?\n",
    "\n",
    "dictionary contains a map of all words (tokens) to its unique id\n",
    "\n",
    "You can create a dictionary \n",
    "- from a paragraph of sentences, \n",
    "- from a text file that contains multiple lines of text and \n",
    "- from multiple such text files contained in a directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [\"The Saudis are preparing a report that will acknowledge that\", \n",
    "             \"Saudi journalist Jamal Khashoggi's death was the result of an\", \n",
    "             \"interrogation that went wrong, one that was intended to lead\", \n",
    "             \"to his abduction from Turkey, according to two sources.\"]\n",
    "\n",
    "documents_2 = [\"One source says the report will likely conclude that\", \n",
    "                \"the operation was carried out without clearance and\", \n",
    "                \"transparency and that those involved will be held\", \n",
    "                \"responsible. One of the sources acknowledged that the\", \n",
    "                \"report is still being prepared and cautioned that\", \n",
    "                \"things could change.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(33 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize(split) the sentences into words\n",
    "texts = [[text for text in doc.split()] for doc in documents]\n",
    "\n",
    "# Create dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "# Get information about the dictionary\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Saudis': 0, 'The': 1, 'a': 2, 'acknowledge': 3, 'are': 4, 'preparing': 5, 'report': 6, 'that': 7, 'will': 8, 'Jamal': 9, \"Khashoggi's\": 10, 'Saudi': 11, 'an': 12, 'death': 13, 'journalist': 14, 'of': 15, 'result': 16, 'the': 17, 'was': 18, 'intended': 19, 'interrogation': 20, 'lead': 21, 'one': 22, 'to': 23, 'went': 24, 'wrong,': 25, 'Turkey,': 26, 'abduction': 27, 'according': 28, 'from': 29, 'his': 30, 'sources.': 31, 'two': 32}\n"
     ]
    }
   ],
   "source": [
    "# Show the word to id map\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_With new documents in the future, it is also possible to update an existing dictionary to include the new words._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_2 = [[text for text in doc.split()] for doc in documents_2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary.add_documents(texts_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(60 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create a Dictionary from one or more text files?\n",
    "\n",
    "You can also create a dictionary from a text file or from a directory of text files.\n",
    "\n",
    "The advantage here is it let’s you read an entire text file __without loading the file in memory__ all at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from smart_open import smart_open\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create gensim dictionary form a single tet file\n",
    "dictionary = corpora.Dictionary(simple_preprocess(line, deacc=True) \n",
    "                                for line in open(r'C:\\Users\\Sky\\Desktop\\SimpliLearn\\NLP_Trainer_PPT_July\\NLP_Trainer_PPT_July\\Projects\\0.1 Assisted Practices\\Lesson 5\\Assisted Practice 1\\nlp_doc.txt', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(416 unique tokens: ['alan', 'although', 'an', 'and', 'article']...)\n"
     ]
    }
   ],
   "source": [
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alan': 0,\n",
       " 'although': 1,\n",
       " 'an': 2,\n",
       " 'and': 3,\n",
       " 'article': 4,\n",
       " 'as': 5,\n",
       " 'be': 6,\n",
       " 'called': 7,\n",
       " 'can': 8,\n",
       " 'clarification': 9,\n",
       " 'computing': 10,\n",
       " 'criterion': 11,\n",
       " 'earlier': 12,\n",
       " 'found': 13,\n",
       " 'from': 14,\n",
       " 'generally': 15,\n",
       " 'history': 16,\n",
       " 'in': 17,\n",
       " 'intelligence': 18,\n",
       " 'is': 19,\n",
       " 'language': 20,\n",
       " 'machinery': 21,\n",
       " 'natural': 22,\n",
       " 'needed': 23,\n",
       " 'nlp': 24,\n",
       " 'now': 25,\n",
       " 'of': 26,\n",
       " 'periods': 27,\n",
       " 'processing': 28,\n",
       " 'proposed': 29,\n",
       " 'published': 30,\n",
       " 'started': 31,\n",
       " 'test': 32,\n",
       " 'the': 33,\n",
       " 'titled': 34,\n",
       " 'turing': 35,\n",
       " 'what': 36,\n",
       " 'which': 37,\n",
       " 'work': 38,\n",
       " 'after': 39,\n",
       " 'alpac': 40,\n",
       " 'authors': 41,\n",
       " 'automatic': 42,\n",
       " 'claimed': 43,\n",
       " 'conducted': 44,\n",
       " 'developed': 45,\n",
       " 'dramatically': 46,\n",
       " 'english': 47,\n",
       " 'expectations': 48,\n",
       " 'experiment': 49,\n",
       " 'failed': 50,\n",
       " 'first': 51,\n",
       " 'five': 52,\n",
       " 'for': 53,\n",
       " 'fulfill': 54,\n",
       " 'fully': 55,\n",
       " 'funding': 56,\n",
       " 'further': 57,\n",
       " 'georgetown': 58,\n",
       " 'had': 59,\n",
       " 'however': 60,\n",
       " 'into': 61,\n",
       " 'involved': 62,\n",
       " 'late': 63,\n",
       " 'little': 64,\n",
       " 'long': 65,\n",
       " 'machine': 66,\n",
       " 'more': 67,\n",
       " 'much': 68,\n",
       " 'or': 69,\n",
       " 'problem': 70,\n",
       " 'progress': 71,\n",
       " 'real': 72,\n",
       " 'reduced': 73,\n",
       " 'report': 74,\n",
       " 'research': 75,\n",
       " 'russian': 76,\n",
       " 'sentences': 77,\n",
       " 'sixty': 78,\n",
       " 'slower': 79,\n",
       " 'solved': 80,\n",
       " 'statistical': 81,\n",
       " 'systems': 82,\n",
       " 'ten': 83,\n",
       " 'than': 84,\n",
       " 'that': 85,\n",
       " 'three': 86,\n",
       " 'to': 87,\n",
       " 'translation': 88,\n",
       " 'until': 89,\n",
       " 'was': 90,\n",
       " 'were': 91,\n",
       " 'when': 92,\n",
       " 'within': 93,\n",
       " 'would': 94,\n",
       " 'year': 95,\n",
       " 'years': 96,\n",
       " 'about': 97,\n",
       " 'almost': 98,\n",
       " 'base': 99,\n",
       " 'between': 100,\n",
       " 'blocks': 101,\n",
       " 'by': 102,\n",
       " 'do': 103,\n",
       " 'eliza': 104,\n",
       " 'emotion': 105,\n",
       " 'example': 106,\n",
       " 'exceeded': 107,\n",
       " 'generic': 108,\n",
       " 'head': 109,\n",
       " 'human': 110,\n",
       " 'hurts': 111,\n",
       " 'information': 112,\n",
       " 'interaction': 113,\n",
       " 'joseph': 114,\n",
       " 'knowledge': 115,\n",
       " 'like': 116,\n",
       " 'might': 117,\n",
       " 'my': 118,\n",
       " 'no': 119,\n",
       " 'notably': 120,\n",
       " 'patient': 121,\n",
       " 'provide': 122,\n",
       " 'provided': 123,\n",
       " 'psychotherapist': 124,\n",
       " 'responding': 125,\n",
       " 'response': 126,\n",
       " 'restricted': 127,\n",
       " 'rogerian': 128,\n",
       " 'say': 129,\n",
       " 'shrdlu': 130,\n",
       " 'simulation': 131,\n",
       " 'small': 132,\n",
       " 'some': 133,\n",
       " 'sometimes': 134,\n",
       " 'startlingly': 135,\n",
       " 'successful': 136,\n",
       " 'system': 137,\n",
       " 'thought': 138,\n",
       " 'using': 139,\n",
       " 'very': 140,\n",
       " 'vocabularies': 141,\n",
       " 'weizenbaum': 142,\n",
       " 'why': 143,\n",
       " 'with': 144,\n",
       " 'working': 145,\n",
       " 'worlds': 146,\n",
       " 'written': 147,\n",
       " 'you': 148,\n",
       " 'your': 149,\n",
       " 'are': 150,\n",
       " 'began': 151,\n",
       " 'carbonell': 152,\n",
       " 'chatterbots': 153,\n",
       " 'computer': 154,\n",
       " 'conceptual': 155,\n",
       " 'cullingford': 156,\n",
       " 'data': 157,\n",
       " 'during': 158,\n",
       " 'examples': 159,\n",
       " 'including': 160,\n",
       " 'jabberwacky': 161,\n",
       " 'lehnert': 162,\n",
       " 'many': 163,\n",
       " 'margie': 164,\n",
       " 'meehan': 165,\n",
       " 'ontologies': 166,\n",
       " 'pam': 167,\n",
       " 'parry': 168,\n",
       " 'plot': 169,\n",
       " 'politics': 170,\n",
       " 'programmers': 171,\n",
       " 'qualm': 172,\n",
       " 'racter': 173,\n",
       " 'sam': 174,\n",
       " 'schank': 175,\n",
       " 'structured': 176,\n",
       " 'talespin': 177,\n",
       " 'this': 178,\n",
       " 'time': 179,\n",
       " 'understandable': 180,\n",
       " 'units': 181,\n",
       " 'wilensky': 182,\n",
       " 'world': 183,\n",
       " 'write': 184,\n",
       " 'algorithms': 185,\n",
       " 'approach': 186,\n",
       " 'attaching': 187,\n",
       " 'based': 188,\n",
       " 'both': 189,\n",
       " 'cache': 190,\n",
       " 'chomskyan': 191,\n",
       " 'common': 192,\n",
       " 'complex': 193,\n",
       " 'comprising': 194,\n",
       " 'computational': 195,\n",
       " 'contains': 196,\n",
       " 'corpus': 197,\n",
       " 'decision': 198,\n",
       " 'decisions': 199,\n",
       " 'discouraged': 200,\n",
       " 'dominance': 201,\n",
       " 'due': 202,\n",
       " 'earliest': 203,\n",
       " 'errors': 204,\n",
       " 'especially': 205,\n",
       " 'existing': 206,\n",
       " 'features': 207,\n",
       " 'focused': 208,\n",
       " 'given': 209,\n",
       " 'gradual': 210,\n",
       " 'grammar': 211,\n",
       " 'hand': 212,\n",
       " 'hard': 213,\n",
       " 'has': 214,\n",
       " 'hidden': 215,\n",
       " 'if': 216,\n",
       " 'increase': 217,\n",
       " 'increasingly': 218,\n",
       " 'input': 219,\n",
       " 'integrated': 220,\n",
       " 'introduced': 221,\n",
       " 'introduction': 222,\n",
       " 'larger': 223,\n",
       " 'law': 224,\n",
       " 'learning': 225,\n",
       " 'lessening': 226,\n",
       " 'linguistics': 227,\n",
       " 'make': 228,\n",
       " 'making': 229,\n",
       " 'markov': 230,\n",
       " 'models': 231,\n",
       " 'moore': 232,\n",
       " 'most': 233,\n",
       " 'multiple': 234,\n",
       " 'on': 235,\n",
       " 'part': 236,\n",
       " 'power': 237,\n",
       " 'probabilistic': 238,\n",
       " 'produce': 239,\n",
       " 'produced': 240,\n",
       " 'recognition': 241,\n",
       " 'reliable': 242,\n",
       " 'rely': 243,\n",
       " 'results': 244,\n",
       " 'revolution': 245,\n",
       " 'robust': 246,\n",
       " 'rules': 247,\n",
       " 'see': 248,\n",
       " 'sets': 249,\n",
       " 'similar': 250,\n",
       " 'soft': 251,\n",
       " 'sort': 252,\n",
       " 'speech': 253,\n",
       " 'starting': 254,\n",
       " 'steady': 255,\n",
       " 'subtasks': 256,\n",
       " 'such': 257,\n",
       " 'tagging': 258,\n",
       " 'then': 259,\n",
       " 'theoretical': 260,\n",
       " 'theories': 261,\n",
       " 'there': 262,\n",
       " 'trees': 263,\n",
       " 'underlies': 264,\n",
       " 'underpinnings': 265,\n",
       " 'unfamiliar': 266,\n",
       " 'up': 267,\n",
       " 'upon': 268,\n",
       " 'use': 269,\n",
       " 'used': 270,\n",
       " 'valued': 271,\n",
       " 'weights': 272,\n",
       " 'whose': 273,\n",
       " 'able': 274,\n",
       " 'advantage': 275,\n",
       " 'all': 276,\n",
       " 'amounts': 277,\n",
       " 'at': 278,\n",
       " 'been': 279,\n",
       " 'calling': 280,\n",
       " 'canada': 281,\n",
       " 'complicated': 282,\n",
       " 'continues': 283,\n",
       " 'corpora': 284,\n",
       " 'corresponding': 285,\n",
       " 'deal': 286,\n",
       " 'depended': 287,\n",
       " 'early': 288,\n",
       " 'effectively': 289,\n",
       " 'european': 290,\n",
       " 'field': 291,\n",
       " 'gone': 292,\n",
       " 'government': 293,\n",
       " 'governmental': 294,\n",
       " 'great': 295,\n",
       " 'ibm': 296,\n",
       " 'implemented': 297,\n",
       " 'languages': 298,\n",
       " 'laws': 299,\n",
       " 'limitation': 300,\n",
       " 'limited': 301,\n",
       " 'major': 302,\n",
       " 'methods': 303,\n",
       " 'multilingual': 304,\n",
       " 'notable': 305,\n",
       " 'occurred': 306,\n",
       " 'official': 307,\n",
       " 'often': 308,\n",
       " 'other': 309,\n",
       " 'parliament': 310,\n",
       " 'proceedings': 311,\n",
       " 'result': 312,\n",
       " 'specifically': 313,\n",
       " 'success': 314,\n",
       " 'successes': 315,\n",
       " 'successively': 316,\n",
       " 'take': 317,\n",
       " 'tasks': 318,\n",
       " 'textual': 319,\n",
       " 'these': 320,\n",
       " 'union': 321,\n",
       " 'where': 322,\n",
       " 'accurate': 323,\n",
       " 'algorithm': 324,\n",
       " 'among': 325,\n",
       " 'amount': 326,\n",
       " 'annotated': 327,\n",
       " 'answers': 328,\n",
       " 'available': 329,\n",
       " 'combination': 330,\n",
       " 'complexity': 331,\n",
       " 'content': 332,\n",
       " 'desired': 333,\n",
       " 'difficult': 334,\n",
       " 'enormous': 335,\n",
       " 'enough': 336,\n",
       " 'entire': 337,\n",
       " 'inferior': 338,\n",
       " 'learn': 339,\n",
       " 'less': 340,\n",
       " 'low': 341,\n",
       " 'non': 342,\n",
       " 'not': 343,\n",
       " 'practical': 344,\n",
       " 'produces': 345,\n",
       " 'recent': 346,\n",
       " 'semi': 347,\n",
       " 'supervised': 348,\n",
       " 'task': 349,\n",
       " 'things': 350,\n",
       " 'typically': 351,\n",
       " 'unsupervised': 352,\n",
       " 'web': 353,\n",
       " 'wide': 354,\n",
       " 'achieve': 355,\n",
       " 'alignment': 356,\n",
       " 'answering': 357,\n",
       " 'approaches': 358,\n",
       " 'areas': 359,\n",
       " 'art': 360,\n",
       " 'became': 361,\n",
       " 'capture': 362,\n",
       " 'changes': 363,\n",
       " 'deep': 364,\n",
       " 'dependency': 365,\n",
       " 'designed': 366,\n",
       " 'directly': 367,\n",
       " 'distinct': 368,\n",
       " 'embeddings': 369,\n",
       " 'emphasizes': 370,\n",
       " 'end': 371,\n",
       " 'entailed': 372,\n",
       " 'fact': 373,\n",
       " 'flurry': 374,\n",
       " 'higher': 375,\n",
       " 'how': 376,\n",
       " 'include': 377,\n",
       " 'instance': 378,\n",
       " 'instead': 379,\n",
       " 'intermediate': 380,\n",
       " 'level': 381,\n",
       " 'may': 382,\n",
       " 'modeling': 383,\n",
       " 'need': 384,\n",
       " 'network': 385,\n",
       " 'neural': 386,\n",
       " 'new': 387,\n",
       " 'nmt': 388,\n",
       " 'obviating': 389,\n",
       " 'others': 390,\n",
       " 'paradigm': 391,\n",
       " 'parsing': 392,\n",
       " 'pipeline': 393,\n",
       " 'popular': 394,\n",
       " 'properties': 395,\n",
       " 'question': 396,\n",
       " 'relying': 397,\n",
       " 'representation': 398,\n",
       " 'semantic': 399,\n",
       " 'separate': 400,\n",
       " 'sequence': 401,\n",
       " 'shift': 402,\n",
       " 'showing': 403,\n",
       " 'smt': 404,\n",
       " 'state': 405,\n",
       " 'steps': 406,\n",
       " 'style': 407,\n",
       " 'substantial': 408,\n",
       " 'techniques': 409,\n",
       " 'term': 410,\n",
       " 'transformations': 411,\n",
       " 'viewed': 412,\n",
       " 'widespread': 413,\n",
       " 'word': 414,\n",
       " 'words': 415}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Token to Id map\n",
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## multiple files?\n",
    "\n",
    "Assuming you have all the text files in the same directory, you need to define a class with an __iter__ method. The __iter__() method should iterate through all the files in a given directory and yield the processed list of word tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReadTxtFiles(object):\n",
    "    def __init__(self, dirname):\n",
    "        self.dirname = dirname\n",
    "\n",
    "    def __iter__(self):\n",
    "        for fname in os.listdir(self.dirname):\n",
    "            for line in open(os.path.join(self.dirname, fname), encoding='latin'):\n",
    "                yield simple_preprocess(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_text_directory = r\"C:\\Users\\Sky\\Desktop\\SimpliLearn\\NLP_Trainer_PPT_July\\NLP_Trainer_PPT_July\\Projects\\0.1 Assisted Practices\\Lesson 5\\Assisted Practice 1\\doc\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = corpora.Dictionary(ReadTxtFiles(path_to_text_directory))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.corpora.dictionary.Dictionary at 0x1c90291fa90>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alan': 0,\n",
       " 'although': 1,\n",
       " 'an': 2,\n",
       " 'and': 3,\n",
       " 'article': 4,\n",
       " 'as': 5,\n",
       " 'be': 6,\n",
       " 'called': 7,\n",
       " 'can': 8,\n",
       " 'clarification': 9,\n",
       " 'computing': 10,\n",
       " 'criterion': 11,\n",
       " 'earlier': 12,\n",
       " 'found': 13,\n",
       " 'from': 14,\n",
       " 'generally': 15,\n",
       " 'history': 16,\n",
       " 'in': 17,\n",
       " 'intelligence': 18,\n",
       " 'is': 19,\n",
       " 'language': 20,\n",
       " 'machinery': 21,\n",
       " 'natural': 22,\n",
       " 'needed': 23,\n",
       " 'nlp': 24,\n",
       " 'now': 25,\n",
       " 'of': 26,\n",
       " 'periods': 27,\n",
       " 'processing': 28,\n",
       " 'proposed': 29,\n",
       " 'published': 30,\n",
       " 'started': 31,\n",
       " 'test': 32,\n",
       " 'the': 33,\n",
       " 'titled': 34,\n",
       " 'turing': 35,\n",
       " 'what': 36,\n",
       " 'which': 37,\n",
       " 'work': 38,\n",
       " 'after': 39,\n",
       " 'alpac': 40,\n",
       " 'authors': 41,\n",
       " 'automatic': 42,\n",
       " 'claimed': 43,\n",
       " 'conducted': 44,\n",
       " 'developed': 45,\n",
       " 'dramatically': 46,\n",
       " 'english': 47,\n",
       " 'expectations': 48,\n",
       " 'experiment': 49,\n",
       " 'failed': 50,\n",
       " 'first': 51,\n",
       " 'five': 52,\n",
       " 'for': 53,\n",
       " 'fulfill': 54,\n",
       " 'fully': 55,\n",
       " 'funding': 56,\n",
       " 'further': 57,\n",
       " 'georgetown': 58,\n",
       " 'had': 59,\n",
       " 'however': 60,\n",
       " 'into': 61,\n",
       " 'involved': 62,\n",
       " 'late': 63,\n",
       " 'little': 64,\n",
       " 'long': 65,\n",
       " 'machine': 66,\n",
       " 'more': 67,\n",
       " 'much': 68,\n",
       " 'or': 69,\n",
       " 'problem': 70,\n",
       " 'progress': 71,\n",
       " 'real': 72,\n",
       " 'reduced': 73,\n",
       " 'report': 74,\n",
       " 'research': 75,\n",
       " 'russian': 76,\n",
       " 'sentences': 77,\n",
       " 'sixty': 78,\n",
       " 'slower': 79,\n",
       " 'solved': 80,\n",
       " 'statistical': 81,\n",
       " 'systems': 82,\n",
       " 'ten': 83,\n",
       " 'than': 84,\n",
       " 'that': 85,\n",
       " 'three': 86,\n",
       " 'to': 87,\n",
       " 'translation': 88,\n",
       " 'until': 89,\n",
       " 'was': 90,\n",
       " 'were': 91,\n",
       " 'when': 92,\n",
       " 'within': 93,\n",
       " 'would': 94,\n",
       " 'year': 95,\n",
       " 'years': 96,\n",
       " 'about': 97,\n",
       " 'almost': 98,\n",
       " 'base': 99,\n",
       " 'between': 100,\n",
       " 'blocks': 101,\n",
       " 'by': 102,\n",
       " 'do': 103,\n",
       " 'eliza': 104,\n",
       " 'emotion': 105,\n",
       " 'example': 106,\n",
       " 'exceeded': 107,\n",
       " 'generic': 108,\n",
       " 'head': 109,\n",
       " 'human': 110,\n",
       " 'hurts': 111,\n",
       " 'information': 112,\n",
       " 'interaction': 113,\n",
       " 'joseph': 114,\n",
       " 'knowledge': 115,\n",
       " 'like': 116,\n",
       " 'might': 117,\n",
       " 'my': 118,\n",
       " 'no': 119,\n",
       " 'notably': 120,\n",
       " 'patient': 121,\n",
       " 'provide': 122,\n",
       " 'provided': 123,\n",
       " 'psychotherapist': 124,\n",
       " 'responding': 125,\n",
       " 'response': 126,\n",
       " 'restricted': 127,\n",
       " 'rogerian': 128,\n",
       " 'say': 129,\n",
       " 'shrdlu': 130,\n",
       " 'simulation': 131,\n",
       " 'small': 132,\n",
       " 'some': 133,\n",
       " 'sometimes': 134,\n",
       " 'startlingly': 135,\n",
       " 'successful': 136,\n",
       " 'system': 137,\n",
       " 'thought': 138,\n",
       " 'using': 139,\n",
       " 'very': 140,\n",
       " 'vocabularies': 141,\n",
       " 'weizenbaum': 142,\n",
       " 'why': 143,\n",
       " 'with': 144,\n",
       " 'working': 145,\n",
       " 'worlds': 146,\n",
       " 'written': 147,\n",
       " 'you': 148,\n",
       " 'your': 149,\n",
       " 'are': 150,\n",
       " 'began': 151,\n",
       " 'carbonell': 152,\n",
       " 'chatterbots': 153,\n",
       " 'computer': 154,\n",
       " 'conceptual': 155,\n",
       " 'cullingford': 156,\n",
       " 'data': 157,\n",
       " 'during': 158,\n",
       " 'examples': 159,\n",
       " 'including': 160,\n",
       " 'jabberwacky': 161,\n",
       " 'lehnert': 162,\n",
       " 'many': 163,\n",
       " 'margie': 164,\n",
       " 'meehan': 165,\n",
       " 'ontologies': 166,\n",
       " 'pam': 167,\n",
       " 'parry': 168,\n",
       " 'plot': 169,\n",
       " 'politics': 170,\n",
       " 'programmers': 171,\n",
       " 'qualm': 172,\n",
       " 'racter': 173,\n",
       " 'sam': 174,\n",
       " 'schank': 175,\n",
       " 'structured': 176,\n",
       " 'talespin': 177,\n",
       " 'this': 178,\n",
       " 'time': 179,\n",
       " 'understandable': 180,\n",
       " 'units': 181,\n",
       " 'wilensky': 182,\n",
       " 'world': 183,\n",
       " 'write': 184,\n",
       " 'algorithms': 185,\n",
       " 'approach': 186,\n",
       " 'attaching': 187,\n",
       " 'based': 188,\n",
       " 'both': 189,\n",
       " 'cache': 190,\n",
       " 'chomskyan': 191,\n",
       " 'common': 192,\n",
       " 'complex': 193,\n",
       " 'comprising': 194,\n",
       " 'computational': 195,\n",
       " 'contains': 196,\n",
       " 'corpus': 197,\n",
       " 'decision': 198,\n",
       " 'decisions': 199,\n",
       " 'discouraged': 200,\n",
       " 'dominance': 201,\n",
       " 'due': 202,\n",
       " 'earliest': 203,\n",
       " 'errors': 204,\n",
       " 'especially': 205,\n",
       " 'existing': 206,\n",
       " 'features': 207,\n",
       " 'focused': 208,\n",
       " 'given': 209,\n",
       " 'gradual': 210,\n",
       " 'grammar': 211,\n",
       " 'hand': 212,\n",
       " 'hard': 213,\n",
       " 'has': 214,\n",
       " 'hidden': 215,\n",
       " 'if': 216,\n",
       " 'increase': 217,\n",
       " 'increasingly': 218,\n",
       " 'input': 219,\n",
       " 'integrated': 220,\n",
       " 'introduced': 221,\n",
       " 'introduction': 222,\n",
       " 'larger': 223,\n",
       " 'law': 224,\n",
       " 'learning': 225,\n",
       " 'lessening': 226,\n",
       " 'linguistics': 227,\n",
       " 'make': 228,\n",
       " 'making': 229,\n",
       " 'markov': 230,\n",
       " 'models': 231,\n",
       " 'moore': 232,\n",
       " 'most': 233,\n",
       " 'multiple': 234,\n",
       " 'on': 235,\n",
       " 'part': 236,\n",
       " 'power': 237,\n",
       " 'probabilistic': 238,\n",
       " 'produce': 239,\n",
       " 'produced': 240,\n",
       " 'recognition': 241,\n",
       " 'reliable': 242,\n",
       " 'rely': 243,\n",
       " 'results': 244,\n",
       " 'revolution': 245,\n",
       " 'robust': 246,\n",
       " 'rules': 247,\n",
       " 'see': 248,\n",
       " 'sets': 249,\n",
       " 'similar': 250,\n",
       " 'soft': 251,\n",
       " 'sort': 252,\n",
       " 'speech': 253,\n",
       " 'starting': 254,\n",
       " 'steady': 255,\n",
       " 'subtasks': 256,\n",
       " 'such': 257,\n",
       " 'tagging': 258,\n",
       " 'then': 259,\n",
       " 'theoretical': 260,\n",
       " 'theories': 261,\n",
       " 'there': 262,\n",
       " 'trees': 263,\n",
       " 'underlies': 264,\n",
       " 'underpinnings': 265,\n",
       " 'unfamiliar': 266,\n",
       " 'up': 267,\n",
       " 'upon': 268,\n",
       " 'use': 269,\n",
       " 'used': 270,\n",
       " 'valued': 271,\n",
       " 'weights': 272,\n",
       " 'whose': 273,\n",
       " 'able': 274,\n",
       " 'advantage': 275,\n",
       " 'all': 276,\n",
       " 'amounts': 277,\n",
       " 'at': 278,\n",
       " 'been': 279,\n",
       " 'calling': 280,\n",
       " 'canada': 281,\n",
       " 'complicated': 282,\n",
       " 'continues': 283,\n",
       " 'corpora': 284,\n",
       " 'corresponding': 285,\n",
       " 'deal': 286,\n",
       " 'depended': 287,\n",
       " 'early': 288,\n",
       " 'effectively': 289,\n",
       " 'european': 290,\n",
       " 'field': 291,\n",
       " 'gone': 292,\n",
       " 'government': 293,\n",
       " 'governmental': 294,\n",
       " 'great': 295,\n",
       " 'ibm': 296,\n",
       " 'implemented': 297,\n",
       " 'languages': 298,\n",
       " 'laws': 299,\n",
       " 'limitation': 300,\n",
       " 'limited': 301,\n",
       " 'major': 302,\n",
       " 'methods': 303,\n",
       " 'multilingual': 304,\n",
       " 'notable': 305,\n",
       " 'occurred': 306,\n",
       " 'official': 307,\n",
       " 'often': 308,\n",
       " 'other': 309,\n",
       " 'parliament': 310,\n",
       " 'proceedings': 311,\n",
       " 'result': 312,\n",
       " 'specifically': 313,\n",
       " 'success': 314,\n",
       " 'successes': 315,\n",
       " 'successively': 316,\n",
       " 'take': 317,\n",
       " 'tasks': 318,\n",
       " 'textual': 319,\n",
       " 'these': 320,\n",
       " 'union': 321,\n",
       " 'where': 322,\n",
       " 'accurate': 323,\n",
       " 'algorithm': 324,\n",
       " 'among': 325,\n",
       " 'amount': 326,\n",
       " 'annotated': 327,\n",
       " 'answers': 328,\n",
       " 'available': 329,\n",
       " 'combination': 330,\n",
       " 'complexity': 331,\n",
       " 'content': 332,\n",
       " 'desired': 333,\n",
       " 'difficult': 334,\n",
       " 'enormous': 335,\n",
       " 'enough': 336,\n",
       " 'entire': 337,\n",
       " 'inferior': 338,\n",
       " 'learn': 339,\n",
       " 'less': 340,\n",
       " 'low': 341,\n",
       " 'non': 342,\n",
       " 'not': 343,\n",
       " 'practical': 344,\n",
       " 'produces': 345,\n",
       " 'recent': 346,\n",
       " 'semi': 347,\n",
       " 'supervised': 348,\n",
       " 'task': 349,\n",
       " 'things': 350,\n",
       " 'typically': 351,\n",
       " 'unsupervised': 352,\n",
       " 'web': 353,\n",
       " 'wide': 354,\n",
       " 'achieve': 355,\n",
       " 'alignment': 356,\n",
       " 'answering': 357,\n",
       " 'approaches': 358,\n",
       " 'areas': 359,\n",
       " 'art': 360,\n",
       " 'became': 361,\n",
       " 'capture': 362,\n",
       " 'changes': 363,\n",
       " 'deep': 364,\n",
       " 'dependency': 365,\n",
       " 'designed': 366,\n",
       " 'directly': 367,\n",
       " 'distinct': 368,\n",
       " 'embeddings': 369,\n",
       " 'emphasizes': 370,\n",
       " 'end': 371,\n",
       " 'entailed': 372,\n",
       " 'fact': 373,\n",
       " 'flurry': 374,\n",
       " 'higher': 375,\n",
       " 'how': 376,\n",
       " 'include': 377,\n",
       " 'instance': 378,\n",
       " 'instead': 379,\n",
       " 'intermediate': 380,\n",
       " 'level': 381,\n",
       " 'may': 382,\n",
       " 'modeling': 383,\n",
       " 'need': 384,\n",
       " 'network': 385,\n",
       " 'neural': 386,\n",
       " 'new': 387,\n",
       " 'nmt': 388,\n",
       " 'obviating': 389,\n",
       " 'others': 390,\n",
       " 'paradigm': 391,\n",
       " 'parsing': 392,\n",
       " 'pipeline': 393,\n",
       " 'popular': 394,\n",
       " 'properties': 395,\n",
       " 'question': 396,\n",
       " 'relying': 397,\n",
       " 'representation': 398,\n",
       " 'semantic': 399,\n",
       " 'separate': 400,\n",
       " 'sequence': 401,\n",
       " 'shift': 402,\n",
       " 'showing': 403,\n",
       " 'smt': 404,\n",
       " 'state': 405,\n",
       " 'steps': 406,\n",
       " 'style': 407,\n",
       " 'substantial': 408,\n",
       " 'techniques': 409,\n",
       " 'term': 410,\n",
       " 'transformations': 411,\n",
       " 'viewed': 412,\n",
       " 'widespread': 413,\n",
       " 'word': 414,\n",
       " 'words': 415,\n",
       " 'coding': 416,\n",
       " 'days': 417,\n",
       " 'devising': 418,\n",
       " 'grammars': 419,\n",
       " 'heuristic': 420,\n",
       " 'set': 421,\n",
       " 'stemming': 422,\n",
       " 'writing': 423,\n",
       " 'analysis': 424,\n",
       " 'annotations': 425,\n",
       " 'automatically': 426,\n",
       " 'calls': 427,\n",
       " 'documents': 428,\n",
       " 'form': 429,\n",
       " 'heavily': 430,\n",
       " 'inference': 431,\n",
       " 'large': 432,\n",
       " 'mid': 433,\n",
       " 'plural': 434,\n",
       " 'possibly': 435,\n",
       " 'relied': 436,\n",
       " 'since': 437,\n",
       " 'so': 438,\n",
       " 'through': 439,\n",
       " 'typical': 440,\n",
       " 'applied': 441,\n",
       " 'certainty': 442,\n",
       " 'classes': 443,\n",
       " 'component': 444,\n",
       " 'different': 445,\n",
       " 'each': 446,\n",
       " 'express': 447,\n",
       " 'feature': 448,\n",
       " 'generated': 449,\n",
       " 'handwritten': 450,\n",
       " 'have': 451,\n",
       " 'included': 452,\n",
       " 'model': 453,\n",
       " 'one': 454,\n",
       " 'only': 455,\n",
       " 'possible': 456,\n",
       " 'producing': 457,\n",
       " 'rather': 458,\n",
       " 'relative': 459,\n",
       " 'they': 460,\n",
       " 'advantages': 461,\n",
       " 'over': 462,\n",
       " 'cases': 463,\n",
       " 'directed': 464,\n",
       " 'effort': 465,\n",
       " 'focus': 466,\n",
       " 'it': 467,\n",
       " 'obvious': 468,\n",
       " 'procedures': 469,\n",
       " 'should': 470,\n",
       " 'whereas': 471,\n",
       " 'accidentally': 472,\n",
       " 'before': 473,\n",
       " 'consuming': 474,\n",
       " 'containing': 475,\n",
       " 'creating': 476,\n",
       " 'erroneous': 477,\n",
       " 'error': 478,\n",
       " 'extremely': 479,\n",
       " 'gracefully': 480,\n",
       " 'handling': 481,\n",
       " 'misspelled': 482,\n",
       " 'omitted': 483,\n",
       " 'prone': 484,\n",
       " 'seen': 485,\n",
       " 'structures': 486,\n",
       " 'annotation': 487,\n",
       " 'become': 488,\n",
       " 'beyond': 489,\n",
       " 'handcrafted': 490,\n",
       " 'hours': 491,\n",
       " 'increases': 492,\n",
       " 'increasing': 493,\n",
       " 'limit': 494,\n",
       " 'made': 495,\n",
       " 'man': 496,\n",
       " 'number': 497,\n",
       " 'particular': 498,\n",
       " 'process': 499,\n",
       " 'requires': 500,\n",
       " 'significant': 501,\n",
       " 'simply': 502,\n",
       " 'supplying': 503,\n",
       " 'unmanageable': 504,\n",
       " 'without': 505,\n",
       " 'worked': 506}"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Token to Id map\n",
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
