{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "from math import *\n",
    "import operator\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option(\"display.max_columns\", 10000)\n",
    "import string\n",
    "\n",
    "# import plotting libraries\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas.plotting import scatter_matrix\n",
    "from matplotlib import style\n",
    "%matplotlib inline \n",
    "\n",
    "import seaborn as sns\n",
    "sns.set(style=\"white\", color_codes=True)\n",
    "sns.set(font_scale=1.5)\n",
    "\n",
    "# For text processing\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import word_tokenize  \n",
    "from nltk.tokenize import sent_tokenize \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import spacy\n",
    "from spacy.lang.en import English\n",
    "\n",
    "from nltk.corpus import wordnet as wn\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "# Importing Gensim\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gensim\n",
    "\n",
    "Gensim is known as a Natural Language Processing package that does ‘Topic Modeling’. But its practically much more than that.\n",
    "\n",
    "It is a great package for processing texts, \n",
    "- working with word vector models (such as Word2Vec, FastText etc) \n",
    "- and for building topic models.\n",
    "\n",
    "The core concepts of gensim are:\n",
    "\n",
    "    Document: some text.\n",
    "    Corpus: a collection of documents.\n",
    "    Dictionary: contains a map of all words (tokens) to its unique id.\n",
    "    Vector: a mathematically convenient representation of a document.\n",
    "    Model: an algorithm for transforming vectors from one representation to another.\n",
    "    \n",
    "## 1. Document\n",
    "\n",
    "In Gensim, a document is an object of the text sequence type (commonly known as str in Python 3). \n",
    "\n",
    "A document could be anything from a short 140 character tweet, a single paragraph (i.e., journal article abstract), a news article, or a book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = \"Human machine interface for lab abc computer applications\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Corpus\n",
    "\n",
    "A corpus is a collection of Document objects. serves 2 roles in Gensim:\n",
    "\n",
    "(1) Input for training a Model. During training, the models use this training corpus to look for common themes and topics, initializing their internal model parameters.\n",
    "\n",
    "Gensim focuses on unsupervised models so that no human intervention, such as costly annotations or tagging documents by hand, is required.\n",
    "\n",
    "(2) Documents to organize. After training, a topic model can be used to extract topics from new documents (documents not seen in the training corpus).\n",
    "\n",
    "Such corpora can be indexed for __Similarity Queries__, queried by semantic similarity, clustered etc.\n",
    "\n",
    "Here is an example corpus. \n",
    "\n",
    "It consists of 9 documents, where each document is a string consisting of a single sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_corpus = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example could be a \n",
    "- list of all the plays written by Shakespeare, \n",
    "- list of all wikipedia articles, or \n",
    "- all tweets by a particular person/object of interest.\n",
    "\n",
    "After collecting our corpus, there are typically __a number of preprocessing__ steps we want to undertake ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a', 'and', 'for', 'in', 'of', 'the', 'to'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set('for a of the and to in in in'.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a set of frequent words\n",
    "stoplist = set('for a of the and to in'.split(' '))\n",
    "\n",
    "# Lowercase each document, split it by white space and filter out stopwords\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "         for document in text_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'machine', 'interface', 'lab', 'abc', 'computer', 'applications'],\n",
       " ['survey', 'user', 'opinion', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'management', 'system'],\n",
       " ['system', 'human', 'system', 'engineering', 'testing', 'eps'],\n",
       " ['relation', 'user', 'perceived', 'response', 'time', 'error', 'measurement'],\n",
       " ['generation', 'random', 'binary', 'unordered', 'trees'],\n",
       " ['intersection', 'graph', 'paths', 'trees'],\n",
       " ['graph', 'minors', 'iv', 'widths', 'trees', 'well', 'quasi', 'ordering'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count word frequencies\n",
    "from collections import defaultdict\n",
    "\n",
    "frequency = defaultdict(int)\n",
    "\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int,\n",
       "            {'human': 2,\n",
       "             'machine': 1,\n",
       "             'interface': 2,\n",
       "             'lab': 1,\n",
       "             'abc': 1,\n",
       "             'computer': 2,\n",
       "             'applications': 1,\n",
       "             'survey': 2,\n",
       "             'user': 3,\n",
       "             'opinion': 1,\n",
       "             'system': 4,\n",
       "             'response': 2,\n",
       "             'time': 2,\n",
       "             'eps': 2,\n",
       "             'management': 1,\n",
       "             'engineering': 1,\n",
       "             'testing': 1,\n",
       "             'relation': 1,\n",
       "             'perceived': 1,\n",
       "             'error': 1,\n",
       "             'measurement': 1,\n",
       "             'generation': 1,\n",
       "             'random': 1,\n",
       "             'binary': 1,\n",
       "             'unordered': 1,\n",
       "             'trees': 3,\n",
       "             'intersection': 1,\n",
       "             'graph': 3,\n",
       "             'paths': 1,\n",
       "             'minors': 2,\n",
       "             'iv': 1,\n",
       "             'widths': 1,\n",
       "             'well': 1,\n",
       "             'quasi': 1,\n",
       "             'ordering': 1})"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "# Only keep words that appear more than once\n",
    "processed_corpus = [[token for token in text iv for text in texts]\n",
    "print(processed_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dictionary\n",
    "\n",
    "In gensim, the dictionary contains a map of all words (tokens) to its unique id.\n",
    "\n",
    "You can create a dictionary from a paragraph of sentences, from a text file that contains multiple lines of text and from multiple such text files contained in a directory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(12 unique tokens: ['computer', 'human', 'interface', 'response', 'survey']...)\n"
     ]
    }
   ],
   "source": [
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Vector\n",
    "\n",
    "Our processed corpus has 12 unique words in it, which means that each document will be represented by a 12-dimensional vector under the __bag-of-words__ model. \n",
    "\n",
    "We can use the dictionary to turn tokenized documents into these 12-dimensional vectors. We can see what these IDs correspond to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'computer': 0,\n",
       " 'human': 1,\n",
       " 'interface': 2,\n",
       " 'response': 3,\n",
       " 'survey': 4,\n",
       " 'system': 5,\n",
       " 'time': 6,\n",
       " 'user': 7,\n",
       " 'eps': 8,\n",
       " 'trees': 9,\n",
       " 'graph': 10,\n",
       " 'minors': 11}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionary.token2id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bag of wrods\n",
    "\n",
    "#### doc2bow(document, allow_update=False, return_missing=False)\n",
    "Convert document (a __list__ of words) into the bag-of-words format = list of (token_id, token_count) 2-tuples. \n",
    "\n",
    "Each word is assumed to be a tokenized and normalized string (either unicode or utf8-encoded). No further preprocessing is done on the words in document; apply tokenization, stemming etc. before calling this method.\n",
    "\n",
    "If _allow_update_ is set, then also update dictionary in the process: create ids for new words. At the same time, update document frequencies – for each word appearing in this document, increase its document frequency (self.dfs) by one.\n",
    "\n",
    "If allow_update is not set, this function is const, aka read-only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 1), (1, 1), (4, 3)]\n"
     ]
    }
   ],
   "source": [
    "new_doc = \"Human computer survey survey survey interaction\"\n",
    "new_vec = dictionary.doc2bow(new_doc.lower().split())\n",
    "print(new_vec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The __first entry__ in each tuple corresponds to the ID of the token in the dictionary, \n",
    "- the __second__ corresponds to the count of this token.\n",
    "\n",
    "Note that “interaction” did not occur in the original corpus and so it was not included in the vectorization. \n",
    "\n",
    "Also note that this vector only contains entries for words that actually appeared in the document. \n",
    "\n",
    "Because any given document will only contain a few words out of the many words in the dictionary, words that do not appear in the vectorization are represented as implicitly zero as a space saving measure.\n",
    "\n",
    "We can convert our entire original corpus to a list of vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'computer'],\n",
       " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'system'],\n",
       " ['system', 'human', 'system', 'eps'],\n",
       " ['user', 'response', 'time'],\n",
       " ['trees'],\n",
       " ['graph', 'trees'],\n",
       " ['graph', 'minors', 'trees'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[(0, 1), (1, 1), (2, 1)],\n",
       " [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
       " [(2, 1), (5, 1), (7, 1), (8, 1)],\n",
       " [(1, 1), (5, 2), (8, 1)],\n",
       " [(3, 1), (6, 1), (7, 1)],\n",
       " [(9, 1)],\n",
       " [(9, 1), (10, 1)],\n",
       " [(9, 1), (10, 1), (11, 1)],\n",
       " [(4, 1), (10, 1), (11, 1)]]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model\n",
    "Now that we have vectorized our corpus we can begin to transform it using models. \n",
    "\n",
    "We use model as an abstract term referring to a transformation from one document representation to another. \n",
    "\n",
    "In gensim documents are represented as vectors so a model can be thought of as a transformation between two vector spaces. \n",
    "\n",
    "The model learns the details of this transformation during training, when it reads the training Corpus.\n",
    "\n",
    "_One simple example of a model is tf-idf_. \n",
    "\n",
    "The tf-idf model transforms vectors from the bag-of-words representation to a vector space where the frequency counts are weighted according to the relative rarity of each word in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, 0.5898341626740045), (11, 0.8075244024440723)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "\n",
    "# train the model\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "\n",
    "# transform the \"system minors\" string\n",
    "words = \"system minors\".lower().split()\n",
    "\n",
    "print(tfidf[dictionary.doc2bow(words)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tfidf model again returns a list of tuples, where the first entry is the token ID and the second entry is the tf-idf weighting. \n",
    "\n",
    "_Note that the ID corresponding to “system” (which occurred 4 times in the original corpus) has been weighted lower than the ID corresponding to “minors” (which only occurred twice)_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import similarities\n",
    "\n",
    "index = similarities.SparseMatrixSimilarity(tfidf[bow_corpus], num_features=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and to query the similarity of our query document query_document against every document in the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.0), (1, 0.32448703), (2, 0.41707572), (3, 0.7184812), (4, 0.0), (5, 0.0), (6, 0.0), (7, 0.0), (8, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "query_document = 'system engineering'.split()\n",
    "\n",
    "query_bow      = dictionary.doc2bow(query_document)\n",
    "\n",
    "sims           = index[tfidf[query_bow]]\n",
    "\n",
    "print(list(enumerate(sims)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Document 3 has a similarity score of 0.718=72%, document 2 has a similarity score of 42% etc. \n",
    "\n",
    "We can make this slightly more readable by sorting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.7184812\n",
      "2 0.41707572\n",
      "1 0.32448703\n",
      "0 0.0\n",
      "4 0.0\n",
      "5 0.0\n",
      "6 0.0\n",
      "7 0.0\n",
      "8 0.0\n"
     ]
    }
   ],
   "source": [
    "for document_number, score in sorted(enumerate(sims), key=lambda x: x[1], reverse=True):\n",
    "    print(document_number, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 0.0), (1, 0.0), (2, 0.0), (3, 0.0), (4, 0.0), (5, 1.0), (6, 0.70710677), (7, 0.5080429), (8, 0.0)]\n"
     ]
    }
   ],
   "source": [
    "query_document = 'trees'.split()\n",
    "\n",
    "query_bow      = dictionary.doc2bow(query_document)\n",
    "\n",
    "sims           = index[tfidf[query_bow]]\n",
    "\n",
    "print(list(enumerate(sims)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 1.0\n",
      "6 0.70710677\n",
      "7 0.5080429\n",
      "0 0.0\n",
      "1 0.0\n",
      "2 0.0\n",
      "3 0.0\n",
      "4 0.0\n",
      "8 0.0\n"
     ]
    }
   ],
   "source": [
    "for document_number, score in sorted(enumerate(sims), key=lambda x: x[1], reverse=True):\n",
    "    print(document_number, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install microsoft C++ build tools \n",
    "# as admin\n",
    "# - pip install spacy\n",
    "# - python -m spacy download en\n",
    "# - pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
